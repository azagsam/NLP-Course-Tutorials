{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf279f2-71f2-4680-ba71-14894c849d10",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fdd4ba-5aa9-4c9a-adbd-d06e7274f41b",
   "metadata": {},
   "source": [
    "\n",
    "<sup>This notebook is a part of Natural Language Processing class at the University of Ljubljana, Faculty for computer and information science. Please contact [boshko.koloski@ijs.si](mailto:boshko.koloski@ijs.si) for any comments.</sub>\n",
    "\n",
    "The core functionality of this notebook is to create a retrieval-augmented generation (RAG) system that enables discussion about the NLP subject setting using the `Mistral-7b-v0.2` model.\n",
    "\n",
    "General-purpose language models can be fine-tuned for common tasks such as sentiment analysis and named entity recognition, which typically do not require additional background knowledge.\n",
    "\n",
    "For more complex and knowledge-intensive tasks, it is possible to construct a language model-based system that accesses external knowledge sources. This approach enhances factual consistency, improves the reliability of responses, and mitigates the issue of 'hallucination.'\n",
    "\n",
    "Researchers have introduced the Retrieval Augmented Generation (RAG) method to address these knowledge-intensive tasks. RAG integrates an information retrieval component with a text generator model, allowing for efficient updates and modifications to its internal knowledge without retraining the entire model.\n",
    "\n",
    "RAG operates by taking an input, retrieving a set of relevant or supporting documents from a source like Wikipedia, and concatenating these documents with the original input prompt. This concatenated context is then fed to the text generator, which produces the final output. This adaptability is crucial for situations where facts may evolve over time, as the static parametric knowledge of traditional large language models (LLMs) can become outdated. RAG circumvents the need for retraining, providing access to the most current information and enabling reliable outputs through retrieval-based generation.\n",
    "\n",
    "RAG requires additional document embeddings and the storage of documents in a database for retrieval purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8061fa90-36fd-48e3-8460-2b69b8b03a23",
   "metadata": {},
   "source": [
    "**In simple terms, RAG is to LLMs what an open-book exam is to humans.**\n",
    "\n",
    "The concept of an open-book exam centers around assessing a student's reasoning abilities rather than their capacity to memorize specific details. In a similar vein, RAG separates factual knowledge from the LLM’s reasoning capabilities. This factual information is stored in an external knowledge source, which is both easily accessible and updatable:\n",
    "\n",
    "- **Parametric knowledge:** Knowledge that is learned during training and implicitly stored within the neural network's weights.\n",
    "- **Non-parametric knowledge:** Information that is stored externally, for example, in a vector database.\n",
    "e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1d1e4-3749-4d44-81c7-281503a853fd",
   "metadata": {},
   "source": [
    "![RAG](RAG.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd1539-a012-4a27-bbcd-751c0e92bede",
   "metadata": {},
   "source": [
    "The RAG workflow consists of:\n",
    "\n",
    "1. **The Retrieve**: The user query is used to retrieve relevant context from an external knowledge source. For this, the user query is embedded using an embedding model into the same vector space as the additional context in the vector database. This enables a similarity search, and the top k closest data objects from the vector database are returned.\n",
    "2. **Augment**: The user query and the retrieved additional context are incorporated into a prompt template.\n",
    "3. **Generate**: Finally, the retrieval-augmented prompt is fed to the LLM.\n",
    "\n",
    "We will use the `langchain` framework to efficiently prompt the LLMs and prepare the RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "704f0052-e986-4137-a6f1-21b1c467268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import requests\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a663ef-a480-43c1-9906-c0ff4d888007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_markdown\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, HTMLHeaderTextSplitter, TokenTextSplitter\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_community.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5d3a4f-1ada-4b5e-96ab-58accfe72919",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = f\"cuda:{torch.cuda.current_device()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ed6e0-43dd-4d3e-9e46-087866f7b41a",
   "metadata": {},
   "source": [
    "Large Language Models are known for their significant computational demands. Typically, the size of a model is determined by multiplying the number of parameters (size) by the precision of these values (data type). To conserve memory, weights can be stored using lower-precision data types through a process known as quantization.\n",
    "\n",
    "**Post-Training Quantization (PTQ)** is a straightforward technique where the weights of an already trained model are converted to a lower precision without necessitating any retraining. Although easy to implement, PTQ can lead to potential performance degradation.We will employ PTQ using the `bitsandbytes` library and will load the model in 4-bit precision, applying double quantization with the `nf4` data type. For more information about quantization, visit [this guide on quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization) )pe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7069ae24-e43e-43b5-887b-07d40e56161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # loading in 4 bit \n",
    "    bnb_4bit_quant_type=\"nf4\", # quantization type\n",
    "    bnb_4bit_use_double_quant=True, # nested quantization \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334f05f8-7dd1-4b04-a37b-89b41717bb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249da3aadb41423d8ee35416b8ffa26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config, # we introduce the bnb config here.\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ecb1a-35f1-4055-a7c0-4245f33cb1ba",
   "metadata": {},
   "source": [
    "We also need to load the tokenizer to transform the text as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d430782-e450-46fa-a23f-940bfd20ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b09a3-5dab-4ef0-803d-6d53e759f7eb",
   "metadata": {},
   "source": [
    "We will use pipelines from Hugging Face to perform the prompting and generation with the Mistral model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bfdcad8-a706-4510-8d19-7c7b22a7c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    #temperature=0.0,  \n",
    "    max_new_tokens=8192,  \n",
    "    repetition_penalty=1.1,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff346e-ff4f-4974-bcb6-4616dcee0457",
   "metadata": {},
   "source": [
    "We will use `langchain` to link the HuggingFace models and the chaining prompting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bae41ae7-74c4-4189-832f-615e297dddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f3c3d-83f8-4ced-bf5b-a0321d01ee56",
   "metadata": {},
   "source": [
    "The core functionality of `langchain` is the creation of templates for prompting via `PromptTemplate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf6211f7-591a-4d70-84c5-d99e0c71db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04195f4e-34e2-47d2-b070-552f369601dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful AI QA assistant. When answering questions, use the context enclosed by triple backquotes if it is relevant.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Reply your answer in markdown format.\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2b5c3cf-edde-4a9c-a792-61f70abda7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template=\"\\nYou are a helpful AI QA assistant. When answering questions, use the context enclosed by triple backquotes if it is relevant.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\nReply your answer in markdown format.\\nQuestion: {question}\\nAnswer:\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02a7815e-ba89-4ef2-be72-57b49d75c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22874416-27d4-4091-ad4b-882fbdcdf439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template=\"\\nYou are a helpful AI QA assistant. When answering questions, use the context enclosed by triple backquotes if it is relevant.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\nReply your answer in markdown format.\\nQuestion: {question}\\nAnswer:\")\n",
       "| HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f3a3263f970>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd75d76-7344-4b83-bc29-8593574d33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the scoring criteria of the NLP course?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f18199a-f5ba-446d-9113-e3d334683f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI QA assistant. When answering questions, use the context enclosed by triple backquotes if it is relevant.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Reply your answer in markdown format.\n",
      "Question: What is the scoring criteria of the NLP course?\n",
      "Answer: I'd be happy to help you with that! However, I need some more context to provide an accurate answer. Could you please specify which NLP (Natural Language Processing) course you are referring to? Different courses may have different scoring criteria. For instance, some courses might focus on grammar and syntax, while others might prioritize semantic understanding or machine learning techniques. If you could provide me with the name or a link to the specific course, I would be glad to look up the details for you. In the meantime, here's a general idea of what you might expect in an NLP course:\n",
      "- Assignments: These typically involve implementing various NLP algorithms and techniques, such as tokenization, stemming, part-of-speech tagging, named entity recognition, sentiment analysis, and text summarization.\n",
      "- Quizzes: These assess your understanding of key concepts and terminology related to NLP.\n",
      "- Midterms and finals: These tests cover a broader range of topics and require a deeper level of knowledge and problem-solving skills.\n",
      "- Projects: These allow you to apply NLP techniques to real-world data sets and develop solutions to complex problems.\n",
      "- Participation: Some courses may also include participation points based on class attendance, engagement in discussions, and contributions to group projects.\n",
      "I hope this information helps! Let me know if you have any other questions or if you can provide me with more context about the specific NLP course you are interested in.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9162f422-94de-42de-afca-e277cb8b41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What do I have to do for the peer review?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3c76908-05dd-473a-a889-884938fca642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI QA assistant. When answering questions, use the context enclosed by triple backquotes if it is relevant.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Reply your answer in markdown format.\n",
      "Question: What do I have to do for the peer review?\n",
      "Answer: Based on the context provided, it appears that you are referring to a peer review process for academic work or a similar context. In general, during a peer review process, you will be expected to submit your work to one or more experts in the field for evaluation and feedback. The specific requirements may vary depending on the guidelines of the organization or institution conducting the review. However, some common tasks include:\n",
      "1. Submitting your work electronically or physically according to the instructions provided.\n",
      "2. Providing any necessary supplementary materials, such as data sets or code repositories.\n",
      "3. Ensuring that all citations and references are properly formatted and cited.\n",
      "4. Responding to reviewers' comments in a timely and professional manner.\n",
      "5. Making any necessary revisions to address reviewers' concerns and improve the quality of your work.\n",
      "It is important to carefully read and follow the instructions provided by the organization or institution conducting the review to ensure that you meet all requirements and deadlines. If you have any specific questions about the peer review process or the requirements, you should contact the organization or institution directly for clarification.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d73c41-884f-40b9-9f55-21b7fcaa3f42",
   "metadata": {},
   "source": [
    "### Enter RAG\n",
    "\n",
    "Next, we will define the vector embeddings of our context. We will use the `sentence-transformers/all-mpnet-base-v2` model to embed the documents and a FAISS vector store to store and later retrieve them. LangChain offers the `HuggingFaceEmbeddings` interface to easily load any model from Hugging Face to serve as the document representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e016460-7ed3-4a94-8514-81e564fe98aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MODEL = \"sentence-transformers/all-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c206ebb-4c2e-41e8-b898-682f3250f90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boshkok/anaconda3/envs/llm_topic/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141fb5d9-fe7a-4057-b916-e86ba3eaa026",
   "metadata": {},
   "source": [
    "The `fetch_websites` function will be used to scrape data from our Google Document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adc89c70-f8e0-4e76-8930-e86ce958c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_websites(sites: list[str]):\n",
    "    docs = []\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        filename = f\"{tmpdir}/site.html\"\n",
    "        for site in sites:\n",
    "            res = requests.get(site)\n",
    "            with open(filename, mode=\"wb\") as fp:\n",
    "                fp.write(res.content)\n",
    "            docs.extend(BSHTMLLoader(filename).load())         \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5197078-0a34-42fb-ae25-dc39a6b905ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_webpage = \"https://docs.google.com/document/u/1/d/e/2PACX-1vRVLq-QPQ-0vWYhj5_TlVSVClXJNTNf1d0CDG59PdxQtl-10h-kVBGlIVQMuZ7YKjtqkyU9iEcAx2zI/pub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b006dada-9e2f-4418-b466-08478d1aaf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = fetch_websites([course_webpage])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "931c49f1-2543-49cb-9bc6-84b28993b24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Natural language processing 2024Objavljeno z Google DokumentiPrijavite zloraboVeč o temNatural language processing 2024Samodejna posodobitev vsakih 5 min.Laboratory work - Spring 2024The main goal of laboratory work is to present the most important aspects of data science in practice and to teach you how to use key tools for a NLP engineer. We especially emphasize on self-paced work, raising standards related to development, replicability, reporting, research, visualizing, etc. Our goal is not to provide exact instructions or \"make robots\" out of participants of this course. Participants will need to try to navigate themselves among data, identify promising leads and extract as much information as possible from the data to present to the others (colleagues, instructors, companies or their superiors).Important linksLab sessions course repository\\xa0(continuously updated, use weekly plan links for latest materials)Books and other materials\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Speech and language processing\\xa0(online draft)\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Python 3 Text Processing with NLTK 3 CookbookIntroduction to Data Science Handbook\\xa0 Razvoj slovenščine v digitalnem okolju\\xa0(February 2023)Previous years NLP course materials\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0NLP course 2021 project reports\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0NLP course 2022 project reports\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0NLP course 2023 project reportsNLP course 2024 projects\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0MarksPeer review\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Peer review submission form (TBA)\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Weekly planThis plan is regularly updated. \\xa0Lab sessions\\xa0are meant to discuss materials and your project ideas. Those proficient in some of the topics covered during the course are expected to help other students during the lab work or in online discussions. Such contributions will also be taken into account. During the lab sessions we will show some DEMOs based on which you will work on your projects. Based on your proposals / new ideas we can adapt the weekly plan and prepare additional materials. All the lab session tutorials will be regularly updated in the Github repository. During the lab sessions we will briefly present each week\\'s topic and then mostly discuss your project ideas and work. You are expected to check/run notebooks before the lab sessions and then ask questions/discuss during the lab sessions. In the repository\\'s README you can also find the recordings of each topic.WeekDescriptionMaterials and links19.2. - 23.2./26.2. - 1.3.Lab work introductionProjects overview Group work and projects application procedureBasic text processingSlovene text processingCourse overview and introduction4.3. - 8.3.Text clusteringText classification Traditional sequence tagging (HMM, MEMM, CRF, ...)Language models, knowledge basesProjects sign up form\\xa0(deadline Friday midnight).Github classroom assignment\\xa0(deadline Friday midnight, only one group member creates a team, exactly three members for a group!).11.3. - 15.3.Neural networks introduction (TensorFlow, Keras)Word embeddings & visualizations (offensive language)RNNs vs. GRUs vs. LSTMs + examplesMultiple simple NN architectures example\\xa0(Google Colab)18.3. - 22.3.Introduction to PyTorchPyTorch LightningSLING tutorial (setup, Singularity, SLURM)First submission (Friday, 23:59)* There will be no lab session Wednesday at 10am. Please attend other groups if possible.25.3. - 29.3.First submission defense (in person)1.4. - 5.4.(Mon. holiday)Transformers, BERT (custom task), BERT (tagging, classification), \\xa0KeyBERT (keyword extraction), TopicBERT (topic modeling) 8.4. - 12.4.Generative and conversational AI15.4. - 19.4.LLMs and new NLP frameworks22.4. - 26.4.Graph neural networks for text processing29.4. - 3.5.(Wed., Thu., Fri. holiday)No lab sessionsSecond submission (Friday, 23:59)6.5. - 10.5.Second submission defense (in person)13.5. - 17.5.Consultations/Project work/Discussions(Please attend the lab sessions and discuss your work and ideas!)(Konferenca DSI 2024: if you attend Hackathon, you get +10 points for the lab part, https://hackathon.si) 20.5. - 24.5.Project work/Online discussions(We will attend the LREC-COLING 2024 conference and will therefore not be at the Faculty premises!)Final submission deadline (Friday, 23:59)\\xa0 \\xa0 \\xa0 IMPORTANT: Put your repositories visibility to public before the deadline or shortly after!Peer review submission deadline (Monday, 27.5.2024, 23:59)\\xa0 \\xa0 \\xa0 Peer review link (each group will get an email with repositories to review)!27.5. - 31.5.No organized lab sessions this weekFinal project presentations - Tuesday, May 30 in P04:\\xa0 \\xa0 Project 1: 1pm \\xa0 \\xa0Project 2: 2pm\\xa0 \\xa0 Project 3: 3pm\\xa0 \\xa0 Project 4 & 5: 4pmCourse obligationsPlease regularly check Weekly plan\\xa0and course announcements for possible changes. You are expected to attend the sessions but you must\\xa0attend the defense sessions. At the assignment defense dates at least one member of a group must be present, otherwise all need to provide their doctor’s justification. At the last assignment all members must be present and also need to understand all parts of the submitted solution.All the work must be submitted using your Github project repository. Submission deadlines are indicated in the table above. Submission defenses will be held during the lab sessions.Students must work in groups of three members! There can exist only one group of two members per project type. The distribution of work between members should be seen by commits within the repository.ObligationDescriptionFinal grade relevance (%)Submission 1Project selection & simple corpus analysis\\xa0 - Group (three members) selection\\xa0 - Report containing Introduction, existing solutions/related work and initial ideas\\xa0 - Well organized repository10Submission 2Initial implementation / baseline with results\\xa0 - Updated Submission 1 parts\\xa0 - Implemented at least one solution with analysis\\xa0 - Future directions and ideas\\xa0 - Well organized repository20Submission 3Final solution and report\\xa0 - Final report incl. analyses and discussions\\xa0 - Fully reproducible repository60Peer reviewEvaluate your peer group\\'s work\\xa0 - Each group will check final submissions of two\\xa0other peer groups having the same topic10Total: 100%Grading criteriaAll the graded work is group work. All the work is graded following the scoring schema below. All the course obligations must be graded positive (i.e., 6 or more) to pass.Use PUBLIC GROUP ID for public communication regarding your group. GROUP ID is your internal id of a group for which marks will be publicly available. ScoringScoring is done relative to achievements of all the participants in the course. Instructions will define the criteria the participants need to address and exactly fulfilling the instructions will result in score 8. All the other scores will be relative to the quality of the submitted work. The role of instructors is to find an appropriate clustering of all the works into 6 clusters. To better illustrate the scoring, schema will be as follows:10 - exceptional: Extraordinary results, quality of work or extremely well structured and justified report. There might still be some very minor possibilities for improvement.Repository is clear and runnable. Report is well organized, results are discussed, visualizations are added value to the text and well prepared. Apart from minimum criteria the group tried multiple of their novel ideas to approach the problem.9 - very good: Above average knowledge presentation with some errors. Same as above - it can be visible that a group had novel ideas but they got out of time or did not finish (polish) everything. The submission has multiple minor flaws.8 - good: Submission of solid work, mainly addressing given instructions only.Group implemented everything suggested by the minimum criteria but not investigated further (not found much related work, did not apply multiple other techniques, ...). 7 - superficial: Below average knowledge and submission of work with errors that show partial understanding.Group implemented everything suggested by the minimum criteria but did not discuss results well, performed simple analyses only, ... Also the report is not well organized, and lacks data for reproducibility.6 - sufficient: Minimum criteria addressed with some major errors and drawbacks.Group was trying to implement minimum criteria (or part only) but their work has many minor flaws or a few major ones. The report also reflects their motivation.5 - insufficient: \\xa0Too much lack of knowledge, too many major errors or no work-effort could be drawn from the submitted work.The group did not address one or more points of the minimum criteria and the report contains major flaws. It can be seen that the group did not invest enough time into the work.Final project preparation guidelines and peer review instructionsSome major remarks that you should keep in mind out are the following regarding final submission:Comment on all the specifics of your algorithms that you use or have designed (i.e. features, hyperparameters, ...). In some cases it is useful to include an image instead of providing long descriptions. When you include graphs/images, they must be readable and provide additional insight (e.g. if there are lots of lines across the image or little space between them, it is not okay). When you report on results, keep them as much as possible in one table, so that a reader can compare different configurations. Also, it is useful to bold the best results. Both images and tables should be self-contained - i.e., together with the caption they need to provide enough information to the reader to understand it\\'s meaning without reading text around.Keep your report concise and try to not submit a report longer than 4 pages + references + appendices. Also, make sure you follow the proposed template.Focus on reporting results and using sensible measures. Try to find examples where your algorithm works better and may not even work at all. Explain why and also justify the differences in approaches that you used. In case previous work exists for your dataset, put the best results of other researchers in your results table (even if your results are much lower).Your submitted work (repository and report) should be structured in a way that your colleagues would be able to understand and re-run everything. Include all dependencies for you projects:In case you have used a non-public (or semi-public) dataset, do not include it in the repository, just put your contact data or protected link to download data/provide other instructions to retrieve data.Datasets that are available elsewhere should be just linked in your report/repository. If you performed additional transformations on datasets, scripts for that should be available in the repository.Some of you used models that take longer to train. You can include those models (maybe just the best one) in the repository or elsewhere and link it. Lastly, check that your repositories are publicly available before the peer review period starts! IMPORTANT:Include links to all dependencies/corpora or include them in the repository, so that anyone can check your work. Also include annotated data (if you manually prepared a corpus) or trained models (if training takes a long time).For anyone that will review your work, it must be as simple as possible to run your code.Peer review instructions:Please find the projects you need to review (see link above).Each group needs to review projects of the same topic they have chosen. Submit your peer review scores in the Google Form (see link above).You will get a score also for your grading, depending on how much (of course by some margin) your grading will be different from the assistant\\'s grading. Follow the scoring criteria as presented above and include feedback to your mark.Final project presentation instructionsEach group will have max. 3 minutes (STRICT)\\xa0to present their project. I will put your report to the projector and you will present it along your report. I propose that you focus on specific interesting part that you will present (e.g., table, graph, figure, ...). The most important aspect to present is:\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0What is the \"take-away message\" of your work? This should be concrete and concise, so that anyone can understand (also a completely lay person).See timetable above for timeslots of your presentation. If you cannot attend, please write to me to get an alternative timeslot.Specific projects informationProject 1: LLM Prompt Strategies for Commonsense-Reasoning Tasks (Aleš): This project aims to explore and compare various prompt strategies to enhance commonsense reasoning in large language models (LLMs). Students will investigate methods such as Chain of Thought (CoT), in-context learning, plan-and-solve techniques, etc., to improve the model\\'s performance on tasks requiring commonsense knowledge. The project will involve designing experiments to evaluate the effectiveness of each strategy, analyzing the models\\' reasoning processes, and understanding how different prompting techniques influence the outcomes.Proposed methodology:Literature review on current prompt strategies and their applications in commonsense reasoning.Selection of a commonsense reasoning dataset (e.g., Winograd Schema Challenge)Design and implementation of experiments to compare the effectiveness of various prompt strategies.Detailed analysis of model responses to identify strengths and weaknesses of each strategy (usage of an HPC is obligatory!). Final report summarizing findings, with recommendations for best practices in prompting for commonsense reasoning tasks.References:Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., ... & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35, 24824-24837.Chen, B., Zhang, Z., Langrené, N., & Zhu, S. (2023). Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. arXiv preprint arXiv:2310.14735.Fernando, C., Banarse, D., Michalewski, H., Osindero, S., & Rocktäschel, T. (2023). Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797.Project 2: Parameter-Efficient Fine-Tuning of Language Models (Aleš): This project focuses on investigating parameter-efficient techniques for fine-tuning large language models, such as Low-Rank Adaptation (LoRA), soft prompts, etc. Students will compare different approaches across various NLP tasks to assess the efficiency and effectiveness of each fine-tuning strategy. The evaluation will consider model performance, computational efficiency, and adaptability to different tasks.Proposed methodology:Reviewing parameter-efficient fine-tuning techniques and selecting appropriate methods for experimentation.Designing experiments to compare learning across multiple NLP tasks. Selecting at least 5 different datasets that cover various natural language understanding skills (commonsense reasoning, coreference resolution, text summarization, etc.) and supervised learning settings (classification & generation).Evaluating the models based on appropriate performance metrics, computational resources required, and ease of adaptation to different tasks. It is obligatory to submit your results publicly to SloBENCH.Writing a comprehensive report that discusses the experimental setup, findings, and recommendations for efficient fine-tuning of language models.References:Xu, L., Xie, H., Qin, S. Z. J., Tao, X., & Wang, F. L. (2023). Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment. arXiv preprint arXiv:2312.12148.Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.Lester, B., Al-Rfou, R., & Constant, N. (2021). The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.Project 3: Cross-Lingual Question Generation (Boshko): This project aims to extend the Doc2Query approach, which utilises a T5 model fine-tuned on the MSMARCO dataset for generating queries from documents, to the domain of question generation in multiple languages. The students will assess the quality of questions generated by the model and its effectiveness across different languages, thereby understanding the challenges and opportunities of applying such models in a cross-linguistic context. The students will then fine-tune the given system on Slovenian datasets and evaluate the outputs.Proposed methodology:Literature Review: Conduct a comprehensive review of existing literature on question generation models, focusing on Doc2Query and its applications, as well as cross-lingual NLP techniques.Dataset Selection and Preparation: \\xa0Select a relevant Slovenian dataset question and answering dataset (e.g. SQuAD) or construct one from a sample of Slovenian news articles.Model Fine-Tuning: Fine-tune the T5 model on the selected datasets, adapting the Doc2Query approach for question generation tasks. It is obligatory to use an HPC.Quality Assessment: Design a framework for evaluating the quality of generated questions, considering factors such as relevance, coherence, linguistic correctness, of the pre-trained and fine-tuned model. It is obligatory to manually check and update 300 examples (QA pair) (try to evaluate up to 100 same examples by all members of a group).Final report summarising the results, highlighting advancements and limitations.References:Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 1, Article 140 (January 2020), 67 pages.Thakur, N., Reimers, N., R ̈uckl ́e, A., Srivastava, A., Gurevych, I.: BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In: Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) (2021)Multilingual Doc2Query https://huggingface.co/doc2query/msmarco-14langs-mt5-base-v1Project 4: Slovenian Instruction-based Corpus Generation (Slavko): Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public. Recently, a number of different very large language models were introduced, such as LaMDA, BLOOM, GPT(-3), Galactica, Mixtral, OPT, ... It is also infeasible to train such models without a powerful GPU infrastructure or large amounts of corpora. Based on these models, text-to-text models were often trained, compared to training specific models per each NLP task, such as text classification, question answering, ... Your task is to get to know LLMs and try to understand their creation from higher levels. Try to prepare large amounts of conversational data in Slovene (this is the focus of this task!) that is correctly organized and of good quality to be fed into fine-tuning a multi-lingual LLM (that supports Slovene). Demonstrate work by adapting a model to fine-tune a conversational agent for Slovene.Proposed methodology:Review usable LLMs, select one that you might use (e.g., within SLING infrastructure, VEGA, Nvidia A100 GPUs).(main goal of the project) Review datasets construction and categorization of instructions for selected Instruce-based LLMs. Prepare a plan for data gathering and identify sources (e.g., med-over.net, slo-tech forum, ...). Write crawlers, ... organize data in a way that is useful for \"fine-tuning\"\\xa0the model. Check papers (e.g., BLOOM\\'s, LLaMa 2\\'s) to get to know, what aspects are important when preparing data.Use the data to adapt an existing model using your data (optional).Report on all your findings in the final report.References:Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe, Training language models to follow instructions with human feedback, https://arxiv.org/abs/2203.02155. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., ... & Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models. https://arxiv.org/abs/2307.09288. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, ... \\xa0et al. (300+ additional authors not shown), BLOOM: A 176B-Parameter Open-Access Multilingual Language Model, https://arxiv.org/abs/2211.05100, model: https://huggingface.co/bigscience/bloom\\xa0Project 5: Unsupervised Domain adaptation for Sentence Classification (Boshko): This project seeks to improve document representation in specialized domains by adapting sentence-transformer models, which, while effective, are not inherently tuned to specific fields. The focus will be on investigating two advanced adaptation techniques: TSDAE (Transformer-based Denoising AutoEncoder) and GPL (generative pseudo labeling). These methods aim to refine the representation space, making it more sensitive and accurate within a given domain. The students will evaluate the effect of the adaptation on the classification result. Proposed methodology:Literature review on sentence transformers, TSDAE and GPL \\xa0to understand their application in information retrieval.Selection of a (Slovenian) classification dataset for domain adaptation experiments (SentiNews,https://www.clarin.si/repository/xmlui/handle/11356/1110\\xa0)..Designing and implementation of experiments to assess the impact of domain adaptation techniques on classification performance. It is obligatory to use an HPC.Detailed analysis of classification results to determine the effectiveness of TSDAE, GPL, and ranking functions.Final report summarizing the findings, with recommendations of feasibility of domain adaptation in information retrieval systems for classification.References:Reimers, Nils, and Gurevych, Iryna. \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\" Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, Nov. 2019, Association for Computational Linguistics. \\xa0Wang, K., Reimers, N., Gurevych, I.: Tsdae: Using transformer-based sequential denoising auto-encoderfor unsupervised sentence embedding learning. In: Findings of the Association for Computational Linguistics: EMNLP 2021. pp. 671–688.Wang, K., Thakur, N., Reimers, N., Gurevych, I.: GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval. In: Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2345–2360. Unsupervised Training for Sentence Transformers (TSDAE) https://www.pinecone.io/learn/series/nlp/unsupervised-training-sentence-transformers/Domain Adaptation with Generative Pseudo-Labeling (GPL) https://www.pinecone.io/learn/series/nlp/gpl/\\xa0Project 6: Qualitative Research on Discussions - text categorization (Slavko): Qualitative discourse analysis is an important way social scientists research human interaction.\\xa0Large language models (LLMs) offer potential for tasks like qualitative discourse analysis, which demand a high level of inter-rater reliability among human “coders” (i.e., qualitative research categorizers). This is an exceedingly labor-intensive task, requiring human coders to fully understand the discussion context, consider each participant’s perspective, and comprehend the sentence’s associations with the previous discussion, as well as shared general knowledge. In this task, you create a model to categorize postings in online discussions, such as in a corpus — an online discussion about the story, “The Lady, or the Tiger?”. We provide a coded dataset with a high inter-rater reliability and a codebook including definitions of each category with examples. Your task is building and training a highly reliable language model for this coding task that generalizes to other online discussions.Proposed methodology:Literature Review: Conduct a comprehensive literature review on discourse analysis or dialogic analysis, focusing on the coding criteria and applied approaches related to NLP.Data Exploration: Explore and understand the provided coded discourse dataset (FINAL DATA (unfiltered as in real-life scenarios)).Fine-tuning Models: Building and fine-tuning LLMs on the provided dataset to predict the discussion category of a posting, considering the discussion context, associations with the previous sentences and the involved participants. It is obligatory to use an HPC.Performance Evaluation: Exploring the metrics that used to evaluate the performance of your built models in discourse analysis. Iteratively comparare & revise your model’s performance compared to human coders (categorizers) and revise based on results. You have the option to implement your own evaluation approaches, or compare your model’s performance with that of other alternative models working on the dataset. Your model will also be tested, for generalizability, on another coded online discussion data set with a different codebook.Define explanations of the categories by an LLM model. Use a separately fine-tuned LLM to generate explanations and qualitatively assess them.Final Report: Delivering a comprehensive report on your findings, emphasizing the effectiveness, innovation, and limitations of your proposed models.References:Gee, J. P. (2010). An introduction to discourse analysis: Theory and method. Routledge.\\xa0Note: pdf is freely available online or asking Dr. Glenn SmithSherry, M. B. (2021). How to facilitate meaningful classroom conversations across disciplines, grade levels, and digital platforms. Rowman & Littlefield Publishers, Inc.Zwaan, R. A., Radvansky, G. A., Hilliard, A. E., & Curiel, J. M. (1998). Constructing multidimensional situation models during reading. Scientific studies of reading, 2(3), 199-220.Li, L., Ma, Z., Fan, L., Lee, S., Yu, H., & Hemphill, L. (2023). ChatGPT in education: A discourse analysis of worries and concerns on social media. arXiv preprint arXiv:2305.02201.Pham, C. M., Hoyle, A., Sun, S., & Iyyer, M. (2023). TopicGPT: A Prompt-based Topic Modeling Framework. arXiv preprint arXiv:2311.01449.Project 7: Conversations with Characters in Stories for Literacy — Quick, Customized Persona Bots from \\xa0novels (Slavko): There is a world-wide literacy crisis (Murray, 2021; OECD, 2015, 2019). Young people hate reading and rarely read recreationally. They fail at high level literacy skills, e.g., evaluating texts for validity and integrating across texts to create personal knowledge. Yet, literacy is vital for educational and professional success, life happiness and societal health. One way to motivate young people to read is through conversational interaction with digital personifications of characters (pedagogical agents or PersonaBots) in novels. LLMs provide possible solutions. Khanmigo offers personaBot ChatGPT text conversations with Jay Gatsby (of the classic novel, The Great Gatsby) and with Obama. However, their offerings are limited. Khanamigo provides no information on development time for personaBots, nor does it offer customized personaBots from user-suggested novels. Quick, customized personaBots, for conversations with characters, based on teacher-suggested novels would be enormously educational. To ensure a personaBot is fully contextualized in the specific context and at the same time within the constraints of token limitations, our suggestion is considering the current Retrieval and indexing techniques (i.e., Retrieval Augmented Generation) or implementing more efficient vector searching or similarity computation approaches. .Proposed methodology:Literature Review: Conduct a comprehensive literature review (and services such as character.ai) on personaBots and pedagogical agents in language arts. Look at theoretical systems that inform PersonaBot\\xa0design, personality theory, situation models, pedagogical agents and more.Explore provided sample stories with example scripts for conversations with characters, e.g., simple xml hard-coded scripts. Also, you will be provided with suggested novels in the public domain as test examples (No (useful) data available, just description and example from IMapBook).Based on formative evaluation, fine-tune models for creating custom PersonaBot conversations with characters from suggested stories and novels.Performance Evaluation: Test your persona bots with sample users with newly suggested stories/novels, with high school or university students, through teleconferences and analyze the transcripts of the conversations. Explore metrics to evaluate the performance of your personaBots - how would the evaluation look like. You have the option to implement your own evaluation approaches, or compare your model’s performance with that of other alternative models working on the dataset.Final Report: Delivering a comprehensive report on your findings, emphasizing the effectiveness, innovation, and limitations of your proposed models for creating customized personaBots based on characters in novels.References:Alaimi, M., Law, E., Pantasdo, K. D., Oudeyer, P. Y., & Sauzeon, H. (2020, April). Pedagogical agents for fostering question-asking skills in children. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems\\xa0(pp. 1-13).Bogaerds-Hazenberg, S. T., Evers-Vermeul, J., & van den Bergh, H. (2022). What textbooks offer and what teachers teach: an analysis of the Dutch reading comprehension curriculum. Reading and writing, 35(7), 1497-1523.Nielen, T. M. J., Smith, G. G., Sikkema-de Jong, M. T., Drobisz, J., van Horne, B., & Bus, A. G. (2018). Digital guidance for susceptible readers: effects on fifth graders’ reading motivation and incidental vocabulary learning, Journal of Educational Computing Research. Goldberg, L. R. (2013). An alternative “description of personality”: The Big-Five factor structure. In Personality and Personality Disorders\\xa0(pp. 34-47). Routledge.Murray, J. (2021). Literacy is inadequate: young children need literacies. International Journal of Early Years Education, 29(1), 1-5.Neuman, Y., Kozhukhov, V., & Vilenchik, D. (2023). Data Augmentation for Modeling Human Personality: The Dexter Machine. arXiv preprint arXiv:2301.08606.OECD. (2015). Beyond PISA 2015: A longer-term strategy of PISA. www.oecd.org/pisa/pisaproducts/Longerterm-strategy-of-PISA.pdfOECD. (2019). PISA 2018 assessment and analytical framework. https://dx.doi.org/10. 1787/b25efab8-en.Papaioannou, I. (2022). Designing coherent and engaging open-domain conversational AI systems\\xa0(Doctoral dissertation, Heriot-Watt University).Zwaan, R. A., Radvansky, G. A., Hilliard, A. E., & Curiel, J. M. (1998). Constructing multidimensional situation models during reading. Scientific studies of reading, 2(3), 199-220. Wu, Z., Wang, Y., Ye, J., Feng, J., Xu, J., Qiao, Y., & Wu, Z. (2023). Openicl: An open-source framework for in-context learning. arXiv preprint arXiv:2303.02913.Project 8: Automatic identification of multiword expressions and definitions generation (Slavko): Understanding the relation between the meaning of words is an important part of comprehending natural language. A lot of works have either focused on analysing lexical semantic relations in word embeddings or probing pretrained language models (LLMs), with some exceptions. Given the rarity of highly multilingual benchmarks, it is unclear to what extent LLMs capture relational knowledge and are able to transfer it across languages. We proposed MultiLexBATS, a multilingual parallel dataset of lexical semantic relations adapted from BATS in 15 languages including low-resource languages, such as Bambara, Lithuanian, and Albanian. We tested LLMs\\' ability to capture analogies across languages, and predict translation targets (first reference). There are considerable differences across relation types and languages. Analyze Slovenian inter annotator agreement and generate definitions. Propose corpus improvement.Proposed methodology:Literature review: Review the initial BATS task corpus, MultiLexBATS paper and corpus. Select an open-source LLM and define useful LLM prompts to generate English and Slovenian definitions (for both annotators). See example for the Bridge corpus.Perform automatic translation Eng-Slo along with definitions generation. Semantically analyze three Slovenian results for each keyword. It is obligatory to use an HPC.Propose dataset adaptation - define error types, dataset cleaning and updates. Show that following MutliLexBATS you can improve results (alternatively you can also replace BLOOM with another LLM - e.g. Mixtral).Final report summarizing the findings and critical evaluation of results.References:Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In 1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings.[BATS corpus] Gladkova, A., Drozd, A., & Matsuoka, S. (2016). Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn’t. In Proceedings of the NAACL-HLT SRW\\xa0(pp. 47–54). San Diego, California, June 12-17, 2016: ACL.MultiLexBATS. Accepted to LREC-COLING 2024. (LINK).MultiLexBATS Corpus. Prepared within the COST NexusLinguarum action, led by prof. Dagmar Gromann. Slovenian part taken by Slavko Žitnik and Timotej Knez, 2023. (LINK).Bridge prompts by Iztok Kosem, 2024 (LINK).Project 9: Natural language inference dataset (DigiLing only, Aleš): The goal of this project is to create a NLI dataset by creating text passages that challenge the model\\'s understanding of entailment, neutrality, and contradiction between pairs of longer texts. Students will use LLMs to generate two-paragraph texts using diverse prompts, analyse the accuracy of the model to follow the instructions and correct (if needed) the generated two-paragraph texts. They will also train a small model and (optionally) apply explanation methods to understand model predictions.Proposed methodology:Studying the construction of SI-NLI dataset and other reference literature. Finding a solution to extend the dataset to longer contexts.Designing creative prompts to generate text passages using LLMs (two paragraphs, each approximately 5 sentences and a clear relation between them - entailment, contradiction, neutral). Each member of a team should produce 50 samples.Manual validation of the generated texts based on their logical relationships (entailment, neutrality, contradiction) and correction of mistakes.Combining samples of all team members into one large dataframe.Training a small model and using it to determine if the created dataset is challenging enough.Compilation of a report detailing the generation process and evaluation process.References:Request a copy of the unpublished paper in which SI-NLI dataset is described via emailYu, F., Zhang, H., & Wang, B. (2023). Nature language reasoning, a survey. arXiv preprint arXiv:2303.14725.Bowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326.', metadata={'source': '/tmp/tmpcx2ibfbv/site.html', 'title': 'Natural language processing 2024'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c164d4-a858-47b6-a654-9b02385b1f62",
   "metadata": {},
   "source": [
    "We loaded the entire course into a single document. Since the sentence transformer can handle only limited sections of text, this might be problematic. Next, we will use the `RecursiveCharacterTextSplitter` to split the document into chunks with a `chunk_size` of 1000 characters and a `chunk_overlap` of 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d47f26e-fa28-4601-8375-df7dc7a512c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20) \n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673b17cb-c0fb-4c81-ae7e-531ff18b9553",
   "metadata": {},
   "source": [
    "Let's see what the chunked document looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a877fcf-dfec-4bd5-9d03-2bd81a901c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Natural language processing 2024Objavljeno z Google DokumentiPrijavite zloraboVeč o temNatural language processing 2024Samodejna posodobitev vsakih 5 min.Laboratory work - Spring 2024The main goal of laboratory work is to present the most important aspects of data science in practice and to teach you how to use key tools for a NLP engineer. We especially emphasize on self-paced work, raising standards related to development, replicability, reporting, research, visualizing, etc. Our goal is not to provide exact instructions or \"make robots\" out of participants of this course. Participants will need to try to navigate themselves among data, identify promising leads and extract as much information as possible from the data to present to the others (colleagues, instructors, companies or their superiors).Important linksLab sessions course repository\\xa0(continuously updated, use weekly plan links for latest materials)Books and other materials\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Speech and language processing\\xa0(online', metadata={'source': '/tmp/tmpcx2ibfbv/site.html', 'title': 'Natural language processing 2024'})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c134359-0c58-4699-aa5d-5612d1bf41ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='processing\\xa0(online draft)\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Python 3 Text Processing with NLTK 3 CookbookIntroduction to Data Science Handbook\\xa0 Razvoj slovenščine v digitalnem okolju\\xa0(February 2023)Previous years NLP course materials\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0NLP course 2021 project reports\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0NLP course 2022 project reports\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0NLP course 2023 project reportsNLP course 2024 projects\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0MarksPeer review\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Peer review submission form (TBA)\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Weekly planThis plan is regularly updated. \\xa0Lab sessions\\xa0are meant to discuss materials and your project ideas. Those proficient in some of the topics covered during the course are expected to help other students during the lab work or in online discussions. Such contributions will also be taken into account. During the lab sessions we will show some DEMOs based on which you will work on your projects. Based on your proposals / new ideas we can adapt the weekly plan and prepare additional materials. All the lab session tutorials will be regularly updated in the Github', metadata={'source': '/tmp/tmpcx2ibfbv/site.html', 'title': 'Natural language processing 2024'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97975fe4-00ed-4746-8fcb-cdcf0b964add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"in the Github repository. During the lab sessions we will briefly present each week's topic and then mostly discuss your project ideas and work. You are expected to check/run notebooks before the lab sessions and then ask questions/discuss during the lab sessions. In the repository's README you can also find the recordings of each topic.WeekDescriptionMaterials and links19.2. - 23.2./26.2. - 1.3.Lab work introductionProjects overview Group work and projects application procedureBasic text processingSlovene text processingCourse overview and introduction4.3. - 8.3.Text clusteringText classification Traditional sequence tagging (HMM, MEMM, CRF, ...)Language models, knowledge basesProjects sign up form\\xa0(deadline Friday midnight).Github classroom assignment\\xa0(deadline Friday midnight, only one group member creates a team, exactly three members for a group!).11.3. - 15.3.Neural networks introduction (TensorFlow, Keras)Word embeddings & visualizations (offensive language)RNNs vs. GRUs vs.\", metadata={'source': '/tmp/tmpcx2ibfbv/site.html', 'title': 'Natural language processing 2024'})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4546820e-b743-4b05-8b4c-9ad5cd2bd5b7",
   "metadata": {},
   "source": [
    "Next, we initialize a vector store. A vector store is a data structure that functions as a vector database, where each document is stored based on its own embedding. We will use the `FAISS` library for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81a9a4a9-befe-4ae9-bedd-b3bd6920c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(all_splits, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e28bd9-6637-46c4-b588-64089bcb2221",
   "metadata": {},
   "source": [
    "Finally, we define a retriever—an object that will handle the **retrieving** part of the RAG pipeline. The retriever receives as arguments the metric by which we search the space and the number of the k-nearest documents (chunks in our case) that we retrieve to present to the studen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "313eff06-8b69-4083-9524-844076bb05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "retreiver = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eaef53-6338-402a-8ea6-fe67ec722232",
   "metadata": {},
   "source": [
    "Next, we modify our prompt template so that it now receives the context (the documents selected by the RAG system) and then generates the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e077d706-5c3a-4703-a419-e2629125ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a helpful AI QA assistant. When answering questions, use the context enclosed by triple backquotes if it is relevant.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Reply your answer in markdown format.\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=PROMPT_TEMPLATE.strip(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b3b2f-c78b-494f-9eab-3316df5fc06d",
   "metadata": {},
   "source": [
    "With the prompt defined, we next set up the `ConversationalRetrievalChain` that will utilize the defined `retriever` and `llm`, following the `PROMPT_TEMPLATE` to extract documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebca475f-aeb5-4aab-bfdf-37ce4ff53c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct complete LLM chain\n",
    "llm_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retreiver,\n",
    "    return_source_documents=False,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template},\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15111aca-e3ed-4bab-8101-d3b52f227d2c",
   "metadata": {},
   "source": [
    "Finally, we create the `answer_question` function that will handle the chain invocation for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b14fab39-6383-4212-8e69-79085829efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, history: dict[str] = None) -> str:\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    response = llm_chain.invoke({\"question\": question, \"chat_history\": history})\n",
    "    answer = response[\"answer\"].split(\"### Answer:\")[-1].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f94ee3a-836d-41db-9b50-0a9024ac186a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To participate in the peer review process, follow these steps:\n",
       "\n",
       "1. Find the projects you need to review based on the given link.\n",
       "2. Evaluate the final submissions of two other peer groups having the same topic.\n",
       "3. Submit your peer review scores in the Google Form provided.\n",
       "4. Receive a score for your grading based on how much your grading differs from the assistant's grading.\n",
       "\n",
       "Remember to follow the scoring criteria and provide constructive feedback in your marks. Use the public group ID for communication regarding your group. All work is group work, and all course obligations must be graded positively to pass."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What do I have to do for the peer review?\"\n",
    "display_markdown(answer_question(question), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78a5a6bd-9d6d-4497-b32e-7a5e85e4e5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Project 7, titled \"Conversations with Characters in Stories for Literacy,\" is a proposal to develop conversational personaBots using Large Language Models (LLMs) to engage young people in reading. The goal is to address the global literacy crisis and help students improve their literacy skills by interacting with digital representations of story characters. The project involves creating custom personaBots for characters from suggested novels, evaluating their performance, and delivering a comprehensive report on the findings."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is Project 7 Conversations with Characters in Stories for Literacy?\"\n",
    "display_markdown(answer_question(question), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10230069-95d9-4eb4-98da-64f0a441ef06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Project Automatic identification of multiword expressions and definitions generation is about understanding the relation between the meaning of words in Natural Language Processing (NLP), specifically focusing on Slovenian language. The project involves analyzing inter-annotator agreement, generating definitions, proposing corpus improvements, performing automatic translation with definition generation, and semantically analyzing the results. The goal is to evaluate the ability of Large Language Models (LLMs) to capture relational knowledge and transfer it across languages using the MultiLexBATS dataset."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is project Automatic identification of multiword expressions and definitions generation about?\"\n",
    "display_markdown(answer_question(question), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db83fc85-982d-46e8-b786-2d2a6bcf2c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The assistant will score the final grade based on the scoring schema provided in the instructions. The schema includes scores for each submission and the overall final grade. The scores are relative to the quality of the submitted work and the achievements of all the participants in the course. The assistant will follow the scoring criteria and provide feedback to the mark. The final grade will be determined after the peer review process and the submission defenses during the lab sessions."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How will the assistent will score the final grade?\"\n",
    "display_markdown(answer_question(question), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5da1d3e3-7768-4c27-9232-716b5e7e66f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To achieve a grade 10, you need to submit exceptional work. This means going beyond the minimum requirements and delivering a project that is extraordinary in terms of its quality, originality, and impact. The project should be well-documented, easy to understand, and fully reproducible. It should also demonstrate a deep understanding of the topic and showcase innovative solutions or approaches. Additionally, the project should include thorough analyses and discussions, and future directions and ideas should be clearly articulated. Lastly, ensure that all dependencies, corpora, or trained models are included in the repository or linked to it, making it as simple as possible for others to run your code."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What needs to be done for a grade 10?\"\n",
    "display_markdown(answer_question(question), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "024b5e55-6617-4add-9ad0-720c768f8f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To achieve a good grade, you need to fully address all the requirements outlined in the instructions. This includes selecting a project topic, creating a well-organized repository, implementing at least one solution with analysis, discussing future directions and ideas, and submitting a final report with thorough analyses and discussions. Make sure your work is publicly available and easy for others to run and understand. Your grading will be based on the quality of your submission compared to other participants in the course."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What do you have to do for a good grade?\"\n",
    "display_markdown(answer_question(question), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f751776d-1eb6-4ab9-87eb-eab46eb9c4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The provided data sets are intended for training and evaluating natural language inference models. These models can then be used as AI assistants or conversational agents that excel in complex reasoning tasks, enabling interaction with humans through intuitive chat interfaces."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Who is the data set for natural language inference intended for?\"\n",
    "display_markdown(answer_question(question), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddb62b-8fe1-43c5-bced-48ff09441765",
   "metadata": {},
   "source": [
    "With the introduction of RAG, the `Mistral` model was able to successfully answer questions about the NLP course.\n",
    "\n",
    "**Exercises:**\n",
    "* Implement a data loader script that can load documents from a folder.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
