{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Wrangling Exercises\n",
    "\n",
    "<sup>This notebook is a part of Natural Language Processing class at the University of Ljubljana, Faculty for computer and information science. Please contact [ales.zagar@fri.uni-lj.si](mailto:ales.zagar@fri.uni-lj.si) for any comments.</sub>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Data wrangling is the process of cleaning, transforming, and organizing data to make it more suitable for analysis. It is a critical step in any data analysis project, as it ensures that the data is accurate, consistent, and complete.\n",
    "\n",
    "These exercises are designed to provide practice in data wrangling skills using a real-world dataset. The dataset used in these exercises is the Slovenian Natural Language Inference dataset (SI-NLI), which contains labeled examples of text pairs with corresponding labels of entailment, contradiction, or neutral.\n",
    "\n",
    "The exercises cover a range of data wrangling techniques, including importing data, performing basic statistics, subsetting observations and variables, creating new variables, grouping data, and combining datasets.\n",
    "\n",
    "By completing these exercises, participants will gain hands-on experience with data wrangling techniques that are essential for data analysis in various fields. They will also develop the skills needed to handle real-world data challenges, such as dealing with missing data, combining datasets, and cleaning data for analysis.\n",
    "\n",
    "Overall, these exercises are an excellent way to improve data wrangling skills and become proficient in handling NLP datasets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get data\n",
    "\n",
    "1. Download SI-NLI from [link](https://www.clarin.si/repository/xmlui/handle/11356/1707).\n",
    "2. Load libraries.\n",
    "3. Import ```train.tsv``` file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'SI-NLI/train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSI-NLI/train.tsv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\t\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m df\u001B[38;5;241m.\u001B[39mhead()\n",
      "File \u001B[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/pandas/io/common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'SI-NLI/train.tsv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('SI-NLI/train.tsv', sep='\\t')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-01T11:16:39.150689Z",
     "start_time": "2024-03-01T11:16:37.995379Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic statistics\n",
    "\n",
    "1. How many examples are in a dataframe?\n",
    "2. How many variables are in a dataframe?\n",
    "3. Count values in the ```label``` column.\n",
    "4. Are there any missing values in the data?\n",
    "5. Count the number of missing values per column.\n",
    "6. Gather all annotator IDs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. How many examples are in a dataframe?\n",
    "len(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-01T11:16:39.169911Z",
     "start_time": "2024-03-01T11:16:39.168199Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. How many variables are in a dataframe?\n",
    "len(df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.168770Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. Count values in the ```label``` column.\n",
    "df['label'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.168896Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. Are there any missing values in the data?\n",
    "any(df.isna())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.169205Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. Count the number of missing values per column.\n",
    "df.isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.169261Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 6. Gather all annotator IDs.\n",
    "a1 = set(df['annotator1_id'])\n",
    "a2 = set(df['annotator2_id'])\n",
    "a3 = set(df['annotator3_id'])\n",
    "\n",
    "print(a1.union(a2).union(a3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.169312Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Subset observations and variables\n",
    "\n",
    "1. Select ```premise``` column and store it in a list.\n",
    "2. Print first 3 rows from the first 3 columns.\n",
    "3. Select ```pair_id```, ```premise```, ```hypothesis```, ```label``` columns and save them into ```train_dataset``` variable.\n",
    "4. Drop ```pair_id``` column.\n",
    "5. Convert all columns to uppercase.\n",
    "6. Replace ```_``` with ```-``` in column names.\n",
    "7. Select rows that belong to the ```neutral``` label.\n",
    "8. Select last 30 rows.\n",
    "9. Select rows with ```hypothesis``` longer than 100 characters.\n",
    "10. Select rows with ```hypothesis``` longer than 100 characters and belong to the ```neutral``` label.\n",
    "11. Select the row with the longest ```hypothesis```.\n",
    "12. Remove rows that contain ```č```, ```š```, ```ž``` in ```premise``` or ```hypothesis```.\n",
    "13. Remove rows that contain at least one missing value.\n",
    "14. Remove the column with the most missing values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Select premise column and store it in a list\n",
    "premise_col = df['premise'].to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.169377Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. Print first 3 rows from the first 3 columns.\n",
    "df.iloc[:3, [0, 1, 2]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.169719Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. Select ```pair_id```, ```premise```, ```hypothesis```, ```label``` columns and save them into ```train_dataset``` variable.\n",
    "train_dataset = df[['pair_id', 'premise', 'hypothesis', 'label']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.169830Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. Drop ```pair_id``` column.\n",
    "df.drop(columns=['pair_id'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.169882Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. Convert all columns to uppercase.\n",
    "df.columns = [i.upper() for i in df.columns]\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-01T11:16:39.172407Z",
     "start_time": "2024-03-01T11:16:39.172071Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 6. Replace ```_``` with ```-``` in column names.\n",
    "df.columns = [i.replace('_', '-') for i in df.columns]\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-01T11:16:39.174442Z",
     "start_time": "2024-03-01T11:16:39.174086Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 7. Select rows that belong to the ```neutral``` label.\n",
    "df = pd.read_csv('SI-NLI/train.tsv', sep='\\t')  # reload\n",
    "df_neutral = df[df['label'] == 'neutral']\n",
    "df_neutral"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.176554Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 8. Select last 30 rows.\n",
    "df.tail(30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.178937Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 9. Select rows with ```hypothesis``` longer than 100 characters.\n",
    "long_hypo_mask = df['hypothesis'].apply(lambda s: len(s) > 100)\n",
    "long_hypo = df[long_hypo_mask]\n",
    "\n",
    "# check\n",
    "print(long_hypo['hypothesis'].apply(len))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.181504Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 10. Select rows with ```hypothesis``` longer than 100 characters and belong to the ```neutral``` label.\n",
    "long_hypo_mask = df['hypothesis'].apply(lambda s: len(s) > 100)\n",
    "neutral_label_mask = df['label'] == 'neutral'\n",
    "final_df = df[long_hypo_mask & neutral_label_mask]\n",
    "final_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.183187Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 11. Select the row with the longest ```hypothesis```.\n",
    "df['hypo_len'] = df['hypothesis'].apply(len)\n",
    "df[df['hypo_len'] == df['hypo_len'].max()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.185411Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 12. Remove rows that contain ```č```, ```š```, ```ž``` in ```premise``` or ```hypothesis```.\n",
    "def check(s):\n",
    "    for c in s.lower():\n",
    "        if c in chars:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chars = ['č', 'š', 'ž']\n",
    "premise_mask = df['premise'].apply(check)\n",
    "hypo_mask = df['hypothesis'].apply(check)\n",
    "df[premise_mask & hypo_mask]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.187517Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 13. Remove rows that contain at least one missing value.\n",
    "df.dropna(axis=0, how='any')  # all rows contain at least one missing value\n",
    "df.dropna(axis=1, how='any')  # this is not true for columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.190001Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 14. Remove the column with the most missing values.\n",
    "col_position = df.isna().sum().argmax()\n",
    "df.drop(columns=[df.columns[col_position]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-01T11:16:39.228932Z",
     "start_time": "2024-03-01T11:16:39.193309Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create new variables\n",
    "\n",
    "#### Basic\n",
    "\n",
    "1. Create integer type variable ```vowel_count_premise``` which stores the number of vowels in a ```premise```. Repeat for ```hypothesis```.\n",
    "2. Create integer type variable with possible values ```1```, ```2```, ```3``` that counts how many annotations a single example received.\n",
    "3. Create boolean type variable ```agreement``` which reflects whether all annotators agreed on the label.\n",
    "4. Create datetime variable that shows when the example was created (randomly sample days from 2022 in a format ```%Y%m%d```).\n",
    "\n",
    "#### Advanced (use classla)\n",
    "5. Create boolean type variable that indicates whether an example contains a proper name.\n",
    "6. Create integer type variable that stores the number of verbs in a ```premise```."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create integer type variable ```vowel_count_premise``` which stores the number of vowels in a ```premise```. Repeat for ```hypothesis```.\n",
    "def count_vowels(s):\n",
    "    n = 0\n",
    "    for c in s.lower():\n",
    "        if c in vowels:\n",
    "            n += 1\n",
    "    return n\n",
    "\n",
    "vowels = {'a', 'e', 'i', 'o', 'u'}\n",
    "df['premise_vowels'] = df['premise'].apply(count_vowels)\n",
    "df['hypothesis_vowels'] = df['hypothesis'].apply(count_vowels)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.196299Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. Create integer type variable with possible values ```1```, ```2```, ```3``` that counts how many annotations a single example received.\n",
    "df['num_of_annotations'] = df[['annotator1_id', 'annotator2_id', 'annotator3_id']].notna().sum(axis=1)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.199546Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. Create boolean type variable ```agreement``` which reflects whether all annotators agreed on the label.\n",
    "values = []\n",
    "for idx, row in df[['annotation_1', 'annotation_2', 'annotation_3']].iterrows():\n",
    "    row = row.dropna()  # drop na from a row\n",
    "    s = set(row.to_dict().values())\n",
    "    if len(s) == 1:\n",
    "        values.append(True)\n",
    "    else:\n",
    "        values.append(False)\n",
    "df['agree'] = values\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.202393Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. Create datetime variable that shows when the example was created (randomly sample days from 2022 in a format ```%Y%m%d```).\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# create dates\n",
    "start = datetime.datetime(2022, 1, 1)\n",
    "end = datetime.datetime(2022, 12, 31)\n",
    "dates = pd.date_range(start, end)\n",
    "# dates = [str(date) for date in dates]\n",
    "\n",
    "df['created'] = [random.choice(dates) for i in range(len(df))]\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.204014Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. Create boolean type variable that indicates whether an example contains a proper name.\n",
    "import classla\n",
    "\n",
    "def check_proper_names(s):\n",
    "    doc = nlp(s)\n",
    "    entities = [ent.type for ent in doc.ents]\n",
    "    if entities:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "nlp = classla.Pipeline('sl', processors='tokenize,ner')\n",
    "df['contains_proper_names_premise'] = df['premise'].apply(check_proper_names)\n",
    "df['contains_proper_names_hypo'] = df['hypothesis'].apply(check_proper_names)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.206187Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 6. Create integer type variable that stores the number of verbs in a ```premise```.\n",
    "def count_verbs(s):\n",
    "    doc = nlp(s)\n",
    "    n = 0\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.upos == 'VERB':\n",
    "                n += 1\n",
    "    return n\n",
    "\n",
    "nlp = classla.Pipeline('sl', processors='tokenize,pos')\n",
    "df['num_of_verbs'] = df['premise'].apply(count_verbs)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.207626Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Group data\n",
    "\n",
    "1. What is the average number of vowels in a ```premise``` per label?\n",
    "2. What is the average length of ```premise``` per label?\n",
    "3. How many examples were created in April?\n",
    "4. How many examples were created on weekends?\n",
    "5. What is the average number of vowels in a ```premise``` per month?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. What is the average number of vowels in a ```premise``` per label?\n",
    "group = df.groupby(by='label')\n",
    "group['premise_vowels'].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.208967Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. What is the average length of ```premise``` per label?\n",
    "df['premise_length'] = df['premise'].apply(len)\n",
    "group = df.groupby(by='label')\n",
    "group['premise_length'].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.210081Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. How many examples were created in April?\n",
    "df['month'] = df['created'].apply(lambda x: x.month)\n",
    "group = df.groupby(by='month')\n",
    "sizes = group.size()\n",
    "sizes.iloc[3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.211571Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. How many examples were created on weekends?\n",
    "df['day'] = df['created'].apply(lambda x: x.weekday())\n",
    "group = df.groupby(by='day')\n",
    "sizes = group.size()\n",
    "sizes.iloc[-2:].sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.212926Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. What is the average number of vowels in a ```premise``` per month?\n",
    "group = df.groupby(by='month')\n",
    "group['premise_length'].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.214472Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combine datasets\n",
    "\n",
    "1. Import dev and test files.\n",
    "2. Combine all three splits into one large dataset.\n",
    "3. What is the average length of ```premise``` per label?\n",
    "4. How many examples each split contains?\n",
    "5. Create a subset that contains exactly the same number of examples per split."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Import dev and test files.\n",
    "train = pd.read_csv('SI-NLI/train.tsv', sep='\\t')\n",
    "train['split'] = ['train']*len(train)\n",
    "dev = pd.read_csv('SI-NLI/dev.tsv', sep='\\t')\n",
    "dev['split'] = ['dev']*len(dev)\n",
    "test = pd.read_csv('SI-NLI/test.tsv', sep='\\t')\n",
    "test['split'] = ['test']*len(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.216815Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. Combine all three splits into one large dataset.\n",
    "df = pd.concat([train, dev, test])\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.220216Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. What is the average length of ```premise``` per label?\n",
    "df['premise_length'] = df['premise'].apply(len)\n",
    "group = df.groupby(by='label')\n",
    "group['premise_length'].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.222921Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. How many examples each split contains?\n",
    "df['label'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.224564Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. Create a subset that contains exactly the same number of examples per split.\n",
    "train_s = train.sample(n=100)\n",
    "dev_s = dev.sample(n=100)\n",
    "test_s = test.sample(n=100)\n",
    "\n",
    "subset = pd.concat([train_s, dev_s, test_s])\n",
    "subset['split'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.225635Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save dataframes\n",
    "\n",
    "1. Save the original dataset to disk in a ```csv``` format.\n",
    "2. Save 3 datasets to disk in a ```tsv``` format: ```contradictions```, ```neutrals```, ```entailments```.\n",
    "3. Assign new IDs to the existing annotator IDs (HINT: use dictionary).\n",
    "4. Create a new dataframe that contains two columns ```old_ID``` and ```new_ID```. Save dataframe in a tabular format on a disk.\n",
    "5. Replace existing annotator IDs with new IDs.\n",
    "6. Save examples to disk in a ```jsonl``` format for each annotator: e.g. ```annotator_A.jsonl```, ```annotator_B.jsonl```, ..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Save the original dataset to disk in a ```csv``` format.\n",
    "df = pd.read_csv('SI-NLI/train.tsv', sep='\\t')\n",
    "df.to_csv('SI-NLI/train.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.227908Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. Save 3 datasets to disk in a ```tsv``` format: ```contradictions```, ```neutrals```, ```entailments```.\n",
    "import os\n",
    "os.makedirs('SI-NLI/labels', exist_ok=True)\n",
    "grouped = df.groupby(by='label')\n",
    "for group_name, group in grouped:\n",
    "    group.to_csv(f'SI-NLI/labels/{group_name}.tsv', sep='\\t', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-01T11:16:39.282416Z",
     "start_time": "2024-03-01T11:16:39.230943Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. Assign new IDs to the existing annotator IDs (HINT: use dictionary).\n",
    "import uuid\n",
    "\n",
    "a1 = set(df['annotator1_id'])\n",
    "a2 = set(df['annotator2_id'])\n",
    "a3 = set(df['annotator3_id'])\n",
    "combined = a1.union(a2).union(a3)\n",
    "mappings = {a: str(uuid.uuid4()) for a in combined}\n",
    "\n",
    "df['annotator1_id'] = df['annotator1_id'].replace(mappings)\n",
    "df['annotator2_id'] = df['annotator2_id'].replace(mappings)\n",
    "df['annotator3_id'] = df['annotator3_id'].replace(mappings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.233706Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. Create a new dataframe that contains two columns ```old_ID``` and ```new_ID```. Save dataframe in a tabular format on a disk.\n",
    "old, new = [], []\n",
    "for k, v in mappings.items():\n",
    "    old.append(k)\n",
    "    new.append(v)\n",
    "\n",
    "map_df = pd.DataFrame({\n",
    "    'old_ID': old,\n",
    "    'new_ID': new\n",
    "})\n",
    "map_df.to_csv('mappings.tsv', index=False, sep='\\t')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.237020Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. Save examples to disk in a ```jsonl``` format for each annotator: e.g. ```annotator_A.jsonl```, ```annotator_B.jsonl```, ...\n",
    "os.makedirs('SI-NLI/annotators')\n",
    "a1 = set(df['annotator1_id'])\n",
    "a2 = set(df['annotator2_id'])\n",
    "a3 = set(df['annotator3_id'])\n",
    "combined = a1.union(a2).union(a3)\n",
    "for a in combined:\n",
    "    subset = df[(df['annotator1_id'] == a) | (df['annotator1_id'] == a) | (df['annotator1_id'] == a)]\n",
    "    subset.to_json(f'SI-NLI/annotators/{a}.jsonl', lines=True, force_ascii=False, orient='records')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2024-03-01T11:16:39.239502Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "nlp",
   "language": "python",
   "display_name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
