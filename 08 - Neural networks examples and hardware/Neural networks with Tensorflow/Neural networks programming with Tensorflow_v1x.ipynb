{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks programming with Tensorflow\n",
    "\n",
    "There exist also some other neural network programming frameworks like [Tensorflow](https://www.tensorflow.org/), for example [PaddlePaddle](http://www.paddlepaddle.org/), [PyTorch](https://pytorch.org/), [Caffe](http://caffe.berkeleyvision.org/), [Keras](https://keras.io/), [Deeplearning4J](https://deeplearning4j.org).\n",
    "\n",
    "Programming in Tensorflow generally consists of the following steps:\n",
    "\n",
    "1. Create tensors or variables (that are not yet executed/evaluated).\n",
    "2. Write operations between tensors and variables.\n",
    "3. Initialize tensors.\n",
    "4. Create a session.\n",
    "5. Run the session (actually running the operations above).\n",
    "\n",
    "### Simple example\n",
    "\n",
    "An example of multiplying two numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(4)\n",
    "b = tf.constant(5)\n",
    "c = tf.multiply(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is not 20 but a tensor without shape attribute and of type *int32*. The result above is the definition of a *computation graph*, which has not been yet run to actually compute the two numbers. To do so, we need to create a new session and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "print(session.run(c))\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got the result of our computation graph.\n",
    "\n",
    "### Parameters for the Tensorflow program\n",
    "\n",
    "Sometimes we do not know all the parameters in advance and would like to additionally feed data later during execution (e.g. training examples). These parameters are known as placeholders and are passed when running as a *feed_dictionary* (`feed_dict` parameter). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read value from a user\n",
    "x = input(\"Enter a number to multiply with 5: \")\n",
    "\n",
    "# Define the computation graph\n",
    "a = tf.placeholder(tf.int32, name = 'a')\n",
    "b = tf.constant(5)\n",
    "c = tf.multiply(a, b)\n",
    "\n",
    "# Execute the computation graph\n",
    "session = tf.Session()\n",
    "print(session.run(c, feed_dict = {a: x}))\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above first constructs the computation graph with a placeholder for values to specify later. At last we run the session in order to execute the computation graph.\n",
    "\n",
    "### Variables computation\n",
    "\n",
    "Now let's compute the loss of a training example defined as:\n",
    "\n",
    "$$loss = \\mathcal{L}(\\hat{y}, y) = (\\hat y^{(i)} - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = tf.constant(67, name='y_hat')            \n",
    "y = tf.constant(69, name='y')                    \n",
    "\n",
    "loss = tf.Variable((y - y_hat)**2, name='loss')  \n",
    "\n",
    "init = tf.global_variables_initializer()         \n",
    "                                                 \n",
    "with tf.Session() as session:                    \n",
    "    session.run(init)                            \n",
    "    print(\"The loss value of a training example is:\", session.run(loss))                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a variable for the loss where we defined a function of other quantities. Before the evaluation we needed to call `tf.global_variables_initializer()` that initialized the variable, which was then evaluated during running the session.\n",
    "\n",
    "To compute the cost of a neural network we need to sum all the losses over all the examples. Tensorflow library already contains prepared functions to compute cost. For example, the function `tf.nn.sigmoid_cross_entropy_with_logits(logits = ...,  labels = ...)` computes the the cross-entropy losses between logits and labels, while logits are outputs of the last unit before the final sigmoid activation.\n",
    "\n",
    "### Linear function\n",
    "\n",
    "Let's compute the following equation: $Z = WX + b$, where $W$ and $X$ are random matrices and b is a random vector. \n",
    "\n",
    "Tensorflow offers a variety of commonly used neural network functions like `tf.sigmoid` or `tf.softmax`. We are now interested to output $sigmoid(Z)$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(1)\n",
    "\n",
    "# Input value\n",
    "x_value = np.random.randn(3,1)\n",
    "    \n",
    "# Neural network graph\n",
    "X = tf.placeholder(tf.float64, name = \"X\")\n",
    "W = tf.Variable(np.random.randn(4,3), name = \"W\")\n",
    "b = tf.Variable(np.random.randn(4,1), name = \"b\")\n",
    "Z = tf.add(tf.matmul(W,X), b)\n",
    "sigmoid = tf.sigmoid(Z)        \n",
    "                            \n",
    "init = tf.global_variables_initializer()    \n",
    "    \n",
    "with tf.Session() as session:    \n",
    "    train_writer = tf.summary.FileWriter('logs/train', session.graph)\n",
    "    \n",
    "    session.run(init)\n",
    "    sigmoid_result, Z_result = session.run([sigmoid, Z], feed_dict = {X: x_value})\n",
    "    print(\"Zs: \\n\", Z_result)\n",
    "    print(\"Sigmoids: \\n\", sigmoid_result)\n",
    "    \n",
    "    train_writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above will return Z values and their sigmoid values. Apart from that, logs for visualization with Tensorboard will be generated in *logs* folder.\n",
    "\n",
    "[Tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard) is a visualization tool that enables the overview of training status or just prints a graph of a given neural network. To run it, call `tensorboard --logdir=logs` and navigate to [http://localhost:6006](http://localhost:6006). \n",
    "\n",
    "The example above generates the following structure:\n",
    "\n",
    "<img src=\"tensorboard.png\" width=\"800px\" />\n",
    "\n",
    "HINT: If you get weird data, restart the notebook kernel, delete files in logs folder and re-run just this part along with Tensorboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Tensorflow training example\n",
    "\n",
    "Let's train a simple linear function, where we optimize RMSE error and use gradient descent optimizer to minimize error. Note that we use `tf.zeros(shape)` for the initialization, alternatively also `tf.ones(shape)` is offered by the Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "epochs = 200\n",
    "\n",
    "# Prepare data\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "\n",
    "# Build graph\n",
    "x = tf.placeholder(tf.float32, shape=[None])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = W * x + b\n",
    "\n",
    "# Update step\n",
    "loss_mse = tf.reduce_mean(tf.square(y - y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss_mse)\n",
    "\n",
    "# Prepare session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Train model\n",
    "print(\"%s\\t%s\\t\\t%s\\t\\t%s\" % (\"Iteration\", \"Loss\", \"W\", \"b\"))\n",
    "for i in range(epochs):\n",
    "    sess.run(train_step, feed_dict={x: x_data, y_: y_data})\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        loss = sess.run(loss_mse, feed_dict={x: x_data, y_: y_data})\n",
    "        print(\"%s\\t\\t%s\\t%s\\t%s\" % (i, loss, sess.run(W)[0], sess.run(b)[0]))\n",
    "\n",
    "print(\"\\nBest fit should be:\\n\\t\\t\\t\\t%s\\t\\t%s\" % (0.1, 0.3))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural networks with Tensorflow\n",
    "\n",
    "Basic recurrent neural network consist of recurrent units that we can classify into basic, GRU and LSTM:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "      <td><img src=\"rnn.png\" /></td>\n",
    "      <td><img src=\"gru.png\" /></td>\n",
    "      <td><img src=\"lstm.png\" /></td>\n",
    "  </tr>\n",
    "</table>\n",
    "<sub>Courtesy of <a href=\"https://medium.com/@saurabh.rathor092/simple-rnn-vs-gru-vs-lstm-difference-lies-in-more-flexible-control-5f33e07b1e57\">https://medium.com/@saurabh.rathor092/simple-rnn-vs-gru-vs-lstm-difference-lies-in-more-flexible-control-5f33e07b1e57</a></sub>\n",
    "\n",
    "**Simple RNN**: There is a simple multiplication of Input (*xt*) and Previous Output (*ht-1*). Passed through *Tanh* activation function. No Gates present.\n",
    "\n",
    "**Gated Recurrent Unit (GRU)**: An Update gate is introduced, to decide whether to pass previous output to next cell or not. Forget gate is nothing but additional Mathematical Operations with a new set of weights.\n",
    "\n",
    "<img src=\"gru_formula.png\" width=\"500px\" />\n",
    "\n",
    "**Long Short Term Memory Unit (LSTM)**: Two more gates are introduced (Forget and Output) in addition to Update gate of GRU. And again as above, these are additional Mathematical Operations on same inputs (xt and ht-1). So overall, LSTM has introduced two math operations having two new sets of weights.\n",
    "\n",
    "<img src=\"lstm_formula.png\" width=\"600px\" />\n",
    "\n",
    "### Recurrent neural networks example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def create_ts(start = '2001', n = 201, freq = 'M'):\n",
    "    rng = pd.date_range(start=start, periods=n, freq=freq)\n",
    "    ts = pd.Series(np.random.uniform(-18, 18, size=len(rng)), rng).cumsum()\n",
    "    return ts\n",
    "\n",
    "ts= create_ts(start = '2001', n = 192, freq = 'M')\n",
    "print(\"Generated data:\")\n",
    "ts.tail(5)\n",
    "\n",
    "ts = create_ts(start = '2001', n = 222)\n",
    "\n",
    "# Left\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(ts.index, ts)\n",
    "plt.plot(ts.index[90:100], ts[90:100], \"b-\", linewidth=3, label=\"A training instance\")\n",
    "plt.title(\"A time series (generated)\", fontsize=14)\n",
    "\n",
    "# Right\n",
    "plt.subplot(122)\n",
    "plt.title(\"A training instance\", fontsize=14)\n",
    "plt.plot(ts.index[90:100], ts[90:100], \"b-\", markersize=8, label=\"instance\")\n",
    "plt.plot(ts.index[91:101], ts[91:101], \"bo\", markersize=10, label=\"target\", markerfacecolor='red')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Time\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = np.array(ts)\n",
    "\n",
    "n_windows = 20   \n",
    "n_input =  1\n",
    "n_output = 1\n",
    "size_train = 201 # 21 points left for testing\n",
    "r_neuron = 120\n",
    "\n",
    "## Split data\n",
    "train = series[:size_train]\n",
    "test = series[size_train:]\n",
    "print(\"Train and test set: \", train.shape, test.shape)\n",
    "\n",
    "def create_batches(df, windows, input, output):\n",
    "    ## Create X         \n",
    "    x_data = train[:size_train-1] \n",
    "    X_batches = x_data.reshape(-1, windows, input) \n",
    "    ## Create y\n",
    "    y_data = train[n_output:size_train] # Lag by the number of output!\n",
    "    y_batches = y_data.reshape(-1, windows, output)\n",
    "    return X_batches, y_batches\n",
    "    \n",
    "X_batches, y_batches = create_batches(df = train,\n",
    "                                      windows = n_windows,\n",
    "                                      input = n_input,\n",
    "                                      output = n_output)\n",
    "X_test, y_test = create_batches(df = test,\n",
    "                                      windows = n_windows,\n",
    "                                      input = n_input,\n",
    "                                      output = n_output)\n",
    "print(\"Training data: \", X_batches.shape, y_batches.shape)\n",
    "print(\"Testing data: \", X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()    \n",
    "\n",
    "## 1. Construct the tensors\n",
    "X = tf.placeholder(tf.float32, [None, n_windows, n_input])   \n",
    "y = tf.placeholder(tf.float32, [None, n_windows, n_output])\n",
    "\n",
    "## 2. create the model\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=r_neuron, activation=tf.nn.relu)   \n",
    "rnn_output, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)    \n",
    "\n",
    "# Tricky part for faster computation\n",
    "stacked_rnn_output = tf.reshape(rnn_output, [-1, r_neuron])          \n",
    "stacked_outputs = tf.layers.dense(stacked_rnn_output, n_output)       \n",
    "outputs = tf.reshape(stacked_outputs, [-1, n_windows, n_output])   \n",
    "\n",
    "## 3. Loss + optimization\n",
    "learning_rate = 0.001  \n",
    " \n",
    "loss = tf.reduce_sum(tf.square(outputs - y))    \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)         \n",
    "training_op = optimizer.minimize(loss)                                          \n",
    "\n",
    "init = tf.global_variables_initializer() \n",
    "\n",
    "iteration = 1500 \n",
    "\n",
    "print(\"\\nTraining:\")\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iters in range(iteration):\n",
    "        sess.run(training_op, feed_dict={X: X_batches, y: y_batches})\n",
    "        if iters % 150 == 0:\n",
    "            mse = loss.eval(feed_dict={X: X_batches, y: y_batches})\n",
    "            print(iters, \"\\tMSE:\", mse)\n",
    "    \n",
    "    y_pred = sess.run(outputs, feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Forecast vs Actual\", fontsize=14)\n",
    "plt.plot(pd.Series(np.ravel(y_test)), \"bo\", markersize=8, label=\"Actual\", color='green')\n",
    "plt.plot(pd.Series(np.ravel(y_pred)), \"r.\", markersize=8, label=\"Forecast\", color='red')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlabel(\"Time\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Useful references:\n",
    "\n",
    "* Tensorflow activation functions: [https://www.tensorflow.org/api_guides/python/nn#Activation_Functions](https://www.tensorflow.org/api_guides/python/nn#Activation_Functions)\n",
    "* \n",
    "* [https://www.tensorflow.org/tutorials/sequences/recurrent](https://www.tensorflow.org/tutorials/sequences/recurrent)\n",
    "* [https://towardsdatascience.com/lstm-by-example-using-tensorflow-feb0c1968537](https://towardsdatascience.com/lstm-by-example-using-tensorflow-feb0c1968537)\n",
    "* [https://www.guru99.com/rnn-tutorial.html](https://www.guru99.com/rnn-tutorial.html) <sub>(Used for the example above)</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:onj] *",
   "language": "python",
   "name": "conda-env-onj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
