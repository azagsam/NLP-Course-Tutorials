{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic text processing, data sources and corpora\n",
    "\n",
    "<sup>This notebook is a part of Natural Language Processing class at the University of Ljubljana, Faculty for computer and information science. Please contact [slavko.zitnik@fri.uni-lj.si](mailto:slavko.zitnik@fri.uni-lj.si) for any comments.</sub>\n",
    "\n",
    "## Short intro to NLTK\n",
    "\n",
    "NLTK library organization ([modules](http://www.nltk.org/py-modindex.html)):\n",
    "\n",
    "| Module |\tShortcuts |\tData Structures\t| Interfaces|\tNLP Pyramid|\n",
    "|---|---|---|---|---|\n",
    "|*nltk.stem*, *nltk.text*, *nltk.tokenize* | *word_tokenize*, *sent_tokenize* |\t*str*, *nltk.Text* => *[str]* |\t*StemmerI*, *TokenizerI* |\t*Morphology* |\n",
    "|*nltk.tag*, *nltk.chunk*|\t*pos_tag*|\t*[str]* => *[(str, tag)]*, *nltk.Tree*|\t*TaggerI*, *ParserI*, *ChunkParserI*\t|Syntax|\n",
    "|*nltk.chunk*, *nltk.sem*|\t*ne_chunk*|\t*nltk.Tree*, *nltk.DependencyGraph*|\t*ParserI*, *ChunkParserI*|\tSemantics|\n",
    "|*nltk.sem.drt*\t|–|\t*Expression*\t|–|\tPragmatics|\n",
    "\n",
    "An example passing the first three levels of the NLP Pyramid:\n",
    "\n",
    "<img src=\"nlp-pyramid.png\" width=\"30%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\alesz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\alesz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\alesz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\alesz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\alesz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alesz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\alesz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alesz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alesz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"gutenberg\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"tagsets\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['John', 'works', 'at', 'OBI', '.']\n",
      "POS tagging: [('John', 'NNP'), ('works', 'VBZ'), ('at', 'IN'), ('OBI', 'NNP'), ('.', '.')]\n",
      "Light parsing: (S (PERSON John/NNP) works/VBZ at/IN (ORGANIZATION OBI/NNP) ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    " \n",
    "text = \"John works at OBI.\" \n",
    " \n",
    "# Morphology Level\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    " \n",
    "# Syntax Level\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(\"POS tagging:\", tagged_tokens)\n",
    " \n",
    "# Semantics Level\n",
    "ner_tree = ne_chunk(tagged_tokens)\n",
    "print(\"Light parsing:\", ner_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with text data, a [Text object](http://www.nltk.org/api/nltk.html#nltk.text.Text) might be useful to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'Monday':\n",
      "april march friday february january\n",
      "\n",
      "Common contexts to a list of words ('August', 'June'):\n",
      "and_and in_the in_and last_the on_the last_when between_and in_to\n",
      "last_that for_shipment in_because ended_shr from_to since_and and_at\n",
      "last_to in_last for_to in_u in_they\n",
      "\n",
      "Contexts of a word 'Monday':\n",
      "Displaying 25 of 240 matches:\n",
      "said . Trade Minister Saleh said on Monday that Indonesia , as the world ' s s\n",
      "Reuters to clarify his statement on Monday in which he said the pact should be\n",
      " the 11 - member CPA which began on Monday . They said producers agreed that c\n",
      "ief Burkhard Junger was arrested on Monday on suspicion of embezzlement and of\n",
      "ween one and 1 . 25 billion dlrs on Monday and Tuesday . The spokesman said Mo\n",
      "ay and Tuesday . The spokesman said Monday ' s float included 500 mln dlrs in \n",
      "s ranged from minus 500 mln dlrs on Monday , when cash letter errors at two ea\n",
      " a deficit on Thursday , Friday and Monday but held excess reserves on the fin\n",
      "ed temporary reserves indirectly on Monday via two billion dlrs of customer re\n",
      "ssets to secure the judgment . Last Monday , the Supreme Court overturned a de\n",
      " Central Bank were higher than last Monday ' s offering , the bank said . The \n",
      " to 98 . 39844 from 98 . 45313 last Monday . Like - dated interbank deposits w\n",
      "e under fresh scrutiny from today ( Monday ), with activity in the European an\n",
      "ose from an unfavorable ruling last Monday by the U . S . Supreme Court in Tex\n",
      "could ,\" Elton said . But following Monday ' s Supreme Court ruling , Texaco '\n",
      "ould come under fresh scrutiny from Monday , with activity in the European and\n",
      "and Trade Minister Rha Woong Bae on Monday to discuss opening South Korean mar\n",
      "oleum Intelligence Weekly , in this Monday ' s edition , said negotiations are\n",
      "nced May 20 , rose another 50 cents Monday to 8 . 375 a share in over - the - \n",
      "ment Marketing were up 12 . 5 cents Monday to 9 . 50 . FIRST UNION & lt ; FUNC\n",
      "tinue to raise U . S . oil prices . Monday , after Texaco confirmed that the p\n",
      "tinue to raise U . S . oil prices . Monday , after Texaco confirmed that the p\n",
      "s . The offer , which will begin on Monday and ends June 30 , requires that th\n",
      "ts two - week public flotation last Monday . The government has carried out ei\n",
      " a U . S . Court in Orlando , Fla . Monday . The company has sought a declarat\n"
     ]
    }
   ],
   "source": [
    "from nltk import Text\n",
    "from nltk.corpus import reuters\n",
    " \n",
    "text = Text(reuters.words())\n",
    " \n",
    "print(\"Similar words to 'Monday':\")\n",
    "text.similar('Monday', 5)\n",
    "\n",
    "print(\"\\nCommon contexts to a list of words ('August', 'June'):\")\n",
    "text.common_contexts(['August', 'June'])\n",
    "\n",
    "print(\"\\nContexts of a word 'Monday':\")\n",
    "text.concordance('Monday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with n-grams (bigrams, trigrams) and collocations extraction (PMI is [Pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best 50 bigrams according to PMI: [('DU', 'PONT'), ('Keng', 'Yaik'), ('Kwik', 'Save'), ('Nihon', 'Keizai'), ('corenes', 'pora'), ('fluidized', 'bed'), ('Akbar', 'Hashemi'), ('Constructions', 'Telephoniques'), ('Elevator', 'Mij'), ('Entre', 'Rios'), ('Graan', 'Elevator'), ('JIM', 'WALTER'), ('Taikoo', 'Shing'), ('der', 'Vorm'), ('di', 'Clemente'), ('Borrowing', 'Requirement'), ('FOOTE', 'MINERAL'), ('Hawker', 'Siddeley'), ('JARDINE', 'MATHESON'), ('PRORATION', 'FACTOR'), ('Wildlife', 'Refuge'), ('Kohlberg', 'Kravis'), ('Almir', 'Pazzionotto'), ('Bankhaus', 'Centrale'), ('Corpus', 'Christi'), ('Kuala', 'Lumpur'), ('Maple', 'Leaf'), ('Stats', 'Oljeselskap'), ('Zoete', 'Wedd'), ('Neutral', 'Zone'), ('Tadashi', 'Kuranari'), ('Drawing', 'Rights'), ('EASTMAN', 'KODAK'), ('Martinez', 'Cuenca'), ('Mathematical', 'Applications'), ('Townsend', 'Thoresen'), ('Sector', 'Borrowing'), ('Hashemi', 'Rafsanjani'), ('Hossein', 'Mousavi'), ('Kitty', 'Hawk'), ('SLAUGHTER', 'GUESSTIMATES'), ('Task', 'Force'), ('Tender', 'Loving'), ('WELLS', 'FARGO'), ('ad', 'hoc'), ('mechanically', 'separated'), ('bleached', 'deodorised'), ('Alejandro', 'Martinez'), ('Het', 'Comite'), ('Paz', 'Estenssoro')]\n",
      "\n",
      "Best 50 trigrams according to PMI: [('Graan', 'Elevator', 'Mij'), ('Sector', 'Borrowing', 'Requirement'), ('Akbar', 'Hashemi', 'Rafsanjani'), ('Lim', 'Keng', 'Yaik'), ('Alejandro', 'Martinez', 'Cuenca'), ('Den', 'Norske', 'Stats'), ('Norske', 'Stats', 'Oljeselskap'), ('Kokusai', 'Denshin', 'Denwa'), ('Special', 'Drawing', 'Rights'), ('Dar', 'es', 'Salaam'), ('FOLLOWING', 'RAINFALL', 'WAS'), ('Duffour', 'et', 'Igon'), ('Tender', 'Loving', 'Care'), ('CATTLE', 'SLAUGHTER', 'GUESSTIMATES'), ('CAMPBELL', 'RED', 'LAKE'), ('Victor', 'Paz', 'Estenssoro'), ('Carter', 'Hawley', 'Hale'), ('Punta', 'del', 'Este'), ('ELEVATOR', 'LOADING', 'WAITING'), ('TIME', 'JOBLESS', 'CLAIMS'), ('Francaise', 'des', 'Petroles'), ('Public', 'Sector', 'Borrowing'), ('Arturo', 'Hernandez', 'Grisanti'), ('Speaker', 'Jim', 'Wright'), ('carrier', 'Kitty', 'Hawk'), ('Archer', 'Daniels', 'Midland'), ('Corning', 'Glass', 'Works'), ('refined', 'bleached', 'deodorised'), ('Grown', 'Cereals', 'Authority'), ('Commissioner', 'Frans', 'Andriessen'), ('RBD', 'PALM', 'OLEIN'), ('RAINFALL', 'THE', 'FOLLOWING'), ('THE', 'FOLLOWING', 'RAINFALL'), ('Kremlin', 'leader', 'Mikhail'), ('Bankhaus', 'Centrale', 'Credit'), ('SANTA', 'FE', 'SOUTHERN'), ('Director', 'Kobena', 'Erbynn'), ('THOUS', 'BUSHELS', 'SOYBEANS'), ('GETS', 'QUALIFIED', 'AUDIT'), ('Denis', 'Bra', 'Kanon'), ('GHANA', 'COCOA', 'PURCHASES'), ('bleached', 'deodorised', 'palm'), ('leader', 'Mikhail', 'Gorbachev'), ('SLAUGHTER', 'GUESSTIMATES', 'Chicago'), ('de', 'Constructions', 'Telephoniques'), ('DISCOUNT', 'WINDOW', 'BORROWINGS'), ('Nil', 'Nil', 'Nil'), ('Fichtel', 'und', 'Sachs'), ('de', 'Zoete', 'Wedd'), ('Home', 'Grown', 'Cereals')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    " \n",
    "# Bigrams\n",
    "finder = BigramCollocationFinder.from_words(nltk.corpus.reuters.words())\n",
    "finder.apply_freq_filter(5)\n",
    "\n",
    "print(\"\\nBest 50 bigrams according to PMI:\", finder.nbest(bigram_measures.pmi, 50))\n",
    " \n",
    "# Trigrams\n",
    "finder = TrigramCollocationFinder.from_words(nltk.corpus.reuters.words())\n",
    "finder.apply_freq_filter(5)\n",
    " \n",
    "print(\"\\nBest 50 trigrams according to PMI:\", finder.nbest(trigram_measures.pmi, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion between different data formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['John', 'works', 'at', 'OBI', '.']\n",
      "\n",
      "Tagged tokens:  [('John', 'NNP'), ('works', 'VBZ'), ('at', 'IN'), ('OBI', 'NNP'), ('.', '.')]\n",
      "\n",
      "Untagged tokens ['John', 'works', 'at', 'OBI', '.']\n",
      "\n",
      "Tagged tokens to strings: ['John/NNP', 'works/VBZ', 'at/IN', 'OBI/NNP', './.']\n",
      "\n",
      "Tagged tokens from strings to tuples: [('John', 'NNP'), ('works', 'VBZ'), ('at', 'IN'), ('OBI', 'NNP'), ('.', '.')]\n",
      "\n",
      "NER tree: (S (PERSON John/NNP) works/VBZ at/IN (ORGANIZATION OBI/NNP) ./.)\n",
      "\n",
      "IOB tagged tree: [('John', 'NNP', 'B-PERSON'), ('works', 'VBZ', 'O'), ('at', 'IN', 'O'), ('OBI', 'NNP', 'B-ORGANIZATION'), ('.', '.', 'O')]\n",
      "\n",
      "Back to tree: (S (PERSON John/NNP) works/VBZ at/IN (ORGANIZATION OBI/NNP) ./.)\n",
      "\n",
      "Tree as CoNLL string:\n",
      " John NNP B-PERSON\n",
      "works VBZ O\n",
      "at IN O\n",
      "OBI NNP B-ORGANIZATION\n",
      ". . O\n",
      "\n",
      "CoNLL string to tree: (S (PERSON John/NNP) works/VBZ at/IN (ORGANIZATION OBI/NNP) ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tag import untag, str2tuple, tuple2str\n",
    "from nltk.chunk import tree2conllstr, conllstr2tree, conlltags2tree, tree2conlltags\n",
    " \n",
    "text = \"John works at OBI.\"\n",
    " \n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens: \", tokens)\n",
    " \n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(\"\\nTagged tokens: \", tagged_tokens)\n",
    " \n",
    "print(\"\\nUntagged tokens\", untag(tagged_tokens))\n",
    " \n",
    "tagged_tokens = [tuple2str(t) for t in tagged_tokens] \n",
    "print(\"\\nTagged tokens to strings:\", tagged_tokens)\n",
    " \n",
    "tagged_tokens = [str2tuple(t) for t in tagged_tokens]\n",
    "print(\"\\nTagged tokens from strings to tuples:\",  tagged_tokens)\n",
    " \n",
    "ner_tree = ne_chunk(tagged_tokens)\n",
    "print(\"\\nNER tree:\", ner_tree)\n",
    " \n",
    "iob_tagged = tree2conlltags(ner_tree)\n",
    "print(\"\\nIOB tagged tree:\", iob_tagged)\n",
    " \n",
    "ner_tree = conlltags2tree(iob_tagged)\n",
    "print(\"\\nBack to tree:\", ner_tree)\n",
    " \n",
    "tree_str = tree2conllstr(ner_tree)\n",
    "print(\"\\nTree as CoNLL string:\\n\", tree_str)\n",
    " \n",
    "ner_tree = conllstr2tree(tree_str, chunk_types=('PERSON', 'ORGANIZATION'))\n",
    "print(\"\\nCoNLL string to tree:\", ner_tree)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence splitting\n",
    "\n",
    "Generally, pretrained models are good enough for tokenization. You might encounter issues using them if you are working with a specific genre of text (e.g. technical with specific abbreviations) or working with an unsupported language.\n",
    "\n",
    "NLTK by default uses *PunktSentenceTokenizer*, which is unsupervised trainable model. A simple scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['All FRI students could get a Msc.', 'in Computer Science.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    " \n",
    "sentence = \"All FRI students could get a Msc. in Computer Science.\"\n",
    "print(\"Sentences:\", sent_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above obviously does not work correctly for us, let's train our model. Default *PunktSentenceTokenizer* is an implementation of [Unsupervised Multilingual Sentence Boundary Detection (Kiss and Strunk (2005)](https://www.aclweb.org/anthology/J06-4003/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from nltk.corpus import gutenberg\n",
    " \n",
    "# 1. Prepare text data\n",
    "text = \"\"\n",
    "for file_id in gutenberg.fileids():\n",
    "    text += gutenberg.raw(file_id)\n",
    "\n",
    "# 2. Train the params\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    " \n",
    "# 3. Instantiate the model\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test again ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized:  ['Mr. James told me Dr.', 'Zitnik is not available today.', 'I will go to his office hours.']\n",
      "\n",
      "Learned abbreviations: {'ed', 'fran', 'l10', 'foh', 'b', 'mss', 'brut', 'wm', 'a.d', 'i.e', 'm.d', 's.w.f', 'dard', 'f', '1.ple', 'doct', 'rosin', 'talb', 'polon', 'g', 'por', 'k.c', 'ult', 'lep', 'g.k', 'lbs', 'ara', 'esq', 'cas', 'hec', 'o.r', 'ger', 'boa', 'gen', 'gho', 'j', 'pol', 'macd', 'ophe', 'var', 'xxx', 'br', 'cai', 'y.sey', 'cyn', \"'mr\", 'p.s', 'phil', 'dut', 'hon', 'laer', 'vol', 'ple', 'cath', 'deci', 'u', 'p', 'jun', 'cin', 'u.s', 'ment', 'sey', 'poe', 'tit', 'luc', 'fla', 'xx', 'guil', 'dec', 'cic', 'cly', 'ami', 'mur', 'w', 'mt', 'ros', 'ang', 'ser', 'clit', 'hag', 'mal', 'ely', 'moo', 'm', 'trans', 'p.m', 'prima', 'rev', 'n', 'bru', 'etc', 'gent', 'finsb', 'fred', 'ibid', 'clo', 'banq', 'pind', 'ant', 'burs', 'hora', 'n.e', 'v.e', 'sw', 'p.h', 'len', 'dag', 'vat', 'cassi', 'octa', 'mac', 'treb', 'stra', 'ro', 'a.s', 'syw', 'cass', 'messa', 'ave', 'cob', 'feb', 'c', 'qu', 'bap', \"'dr\", \"'w\", 'mes', 'oct', 'k', 'calp', 'osr', 'cop', 'malc', 'm.p', 'amb', 'macb'}\n",
      "\n",
      "Split decisions debugging:\n",
      "{'break_decision': False,\n",
      " 'collocation': True,\n",
      " 'period_index': 2,\n",
      " 'reason': 'known collocation (both words)',\n",
      " 'text': 'Mr. James',\n",
      " 'type1': 'mr.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'james',\n",
      " 'type2_is_sent_starter': False,\n",
      " 'type2_ortho_contexts': {'MID-UC', 'UNK-UC', 'BEG-UC'},\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "==============================\n",
      "{'break_decision': True,\n",
      " 'collocation': False,\n",
      " 'period_index': 20,\n",
      " 'reason': 'default decision',\n",
      " 'text': 'Dr. Zitnik',\n",
      " 'type1': 'dr.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'zitnik',\n",
      " 'type2_is_sent_starter': False,\n",
      " 'type2_ortho_contexts': set(),\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "==============================\n",
      "{'break_decision': True,\n",
      " 'collocation': False,\n",
      " 'period_index': 51,\n",
      " 'reason': 'default decision',\n",
      " 'text': 'today. I',\n",
      " 'type1': 'today.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'i',\n",
      " 'type2_is_sent_starter': True,\n",
      " 'type2_ortho_contexts': {'BEG-LC',\n",
      "                          'BEG-UC',\n",
      "                          'MID-LC',\n",
      "                          'MID-UC',\n",
      "                          'UNK-LC',\n",
      "                          'UNK-UC'},\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "sentences = \"Mr. James told me Dr. Zitnik is not available today. I will go to his office hours.\"\n",
    " \n",
    "print(\"Tokenized: \", tokenizer.tokenize(sentences))\n",
    "print(\"\\nLearned abbreviations:\", tokenizer._params.abbrev_types)\n",
    " \n",
    "from pprint import pprint\n",
    "print(\"\\nSplit decisions debugging:\")\n",
    "for decision in tokenizer.debug_decisions(sentences):\n",
    "    pprint(decision)\n",
    "    print('=' * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually add abbreviations or other parameters to update the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: ['All FRI students could get a Msc. in Computer Science.']\n",
      "Tokenized:  ['Mr. James told me Dr. Zitnik is not available today.', 'I will go to his office hours.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer._params.abbrev_types.add('msc')\n",
    "tokenizer._params.abbrev_types.add('dr')\n",
    "\n",
    "sentence = \"All FRI students could get a Msc. in Computer Science.\"\n",
    "print(\"Tokenized:\", tokenizer.tokenize(sentence))\n",
    "\n",
    "sentences = \"Mr. James told me Dr. Zitnik is not available today. I will go to his office hours.\"\n",
    "print(\"Tokenized: \", tokenizer.tokenize(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text processing tools\n",
    "\n",
    "### Stemming\n",
    "\n",
    "> In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.\n",
    "\n",
    "One of the first stemmers was designed by [Martin Porter](https://tartarus.org/martin/PorterStemmer/) (*M.F. Porter, 1980, An algorithm for suffix stripping, Program, 14(3), pp. 130−137*), which is a suffix stripping algorithm and most widely used. Later he continued his work and presented his improved work for English and for some other languages with the Snowball stemmer.\n",
    "\n",
    "The NLTK includes some stemmer implementations - Porter, Lancaster, Snowball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'this'),\n",
       " ('young', 'young'),\n",
       " ('gentlewoman', 'gentlewoman'),\n",
       " ('had', 'had'),\n",
       " ('a', 'a'),\n",
       " ('fathero', 'fathero'),\n",
       " ('that', 'that'),\n",
       " ('had', 'had'),\n",
       " ('how', 'how'),\n",
       " ('sad', 'sad'),\n",
       " ('a', 'a'),\n",
       " ('passage', 'passag'),\n",
       " ('tiswhose', 'tiswhos'),\n",
       " ('skill', 'skill'),\n",
       " ('was', 'was'),\n",
       " ('almost', 'almost'),\n",
       " ('as', 'as'),\n",
       " ('great', 'great'),\n",
       " ('as', 'as'),\n",
       " ('his', 'his'),\n",
       " ('honesty', 'honesti'),\n",
       " ('had', 'had'),\n",
       " ('it', 'it'),\n",
       " ('stretched', 'stretch'),\n",
       " ('so', 'so'),\n",
       " ('far', 'far'),\n",
       " ('would', 'would'),\n",
       " ('have', 'have'),\n",
       " ('made', 'made'),\n",
       " ('nature', 'natur'),\n",
       " ('immortal', 'immort'),\n",
       " ('and', 'and'),\n",
       " ('death', 'death'),\n",
       " ('should', 'should'),\n",
       " ('have', 'have'),\n",
       " ('play', 'play'),\n",
       " ('for', 'for'),\n",
       " ('lack', 'lack'),\n",
       " ('of', 'of'),\n",
       " ('work', 'work'),\n",
       " ('would', 'would'),\n",
       " ('for', 'for'),\n",
       " ('the', 'the'),\n",
       " ('kings', 'king'),\n",
       " ('sake', 'sake'),\n",
       " ('he', 'he'),\n",
       " ('were', 'were'),\n",
       " ('living', 'live'),\n",
       " ('i', 'i'),\n",
       " ('think', 'think')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "# Prepare text\n",
    "def getTokens():\n",
    "   with open('shakespeare.txt', 'r') as shakes:\n",
    "    text = shakes.read().lower()\n",
    "    # remove punctuation\n",
    "    table = text.maketrans({key: None for key in string.punctuation})\n",
    "    text = text.translate(table)      \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Do stemming\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stems = [(token, stemmer.stem(token)) for token in getTokens()]\n",
    "stems[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation\n",
    "\n",
    "> Lemmatisation (or lemmatization) in linguistics is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n",
    "\n",
    "> In computational linguistics, lemmatisation is the algorithmic process of determining the lemma for a given word. Since the process may involve complex tasks such as understanding context and determining the part of speech of a word in a sentence (requiring, for example, knowledge of the grammar of a language) it can be a hard task to implement a lemmatiser for a new language.\n",
    "\n",
    "> In many languages, words appear in several inflected forms. For example, in English, the verb 'to walk' may appear as 'walk', 'walked', 'walks', 'walking'. The base form, 'walk', that one might look up in a dictionary, is called the lemma for the word.\n",
    "\n",
    "> Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in getTokens()]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the differences between lemmas and stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above finds a lemma expecting a noun by default, so we should also include a part-of-speech information, e.g., `lemmatizer.lemmatize(\"walking\", pos='v')` for verbs - the default parameter is noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try to lemmatise is, are, was, were without and with pos parameter. Is there a difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech tagging\n",
    "\n",
    "> A part of speech (abbreviated form: PoS or POS) is a category of words (or, more generally, of lexical items) which have similar grammatical properties. Words that are assigned to the same part of speech generally display similar behavior in terms of syntax—they play similar roles within the grammatical structure of sentences—and sometimes in terms of morphology, in that they undergo inflection for similar properties. Commonly listed English parts of speech are noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article or determiner.\n",
    "\n",
    "In NLTK, there exist some implementations of taggers and possible tagsets ([source](http://www.nltk.org/_modules/nltk/tag.html#pos_tag))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posTagged = nltk.pos_tag(getTokens())\n",
    "posTagged[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the Penn Treebank tags are used ([tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset(\"NNP\") # Meaning of a specific tag with examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents comparison using TF-IDF\n",
    "\n",
    "TF-IDF is a common metric in information retrieval which weights a term with respect to a list of documents.\n",
    "\n",
    "> In information retrieval, tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "> Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification.\n",
    "\n",
    "The metric is calculated as follows:\n",
    "\n",
    "$$\\textrm{TF}(term) = \\frac{\\textrm{Number of times term appears in a document}}{\\textrm{Total number of terms in the document}}$$\n",
    "\n",
    "$$\\textrm{IDF}(term) = \\frac{\\textrm{Total number of documents}}{\\textrm{Number of documents with the term in it}}$$\n",
    "\n",
    "$$\\textrm{TF-IDF}(term) = \\textrm{TF}(term) * \\log_e(\\textrm{IDF}(term))$$\n",
    "\n",
    "Computing TF-IDF scores for given documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectors (each column is a document):\n",
      "[[0.         0.         0.42068099 0.        ]\n",
      " [0.         0.         0.         0.50867187]\n",
      " [0.         0.         0.42068099 0.        ]\n",
      " [0.44809973 0.39205255 0.26851522 0.        ]\n",
      " [0.70203482 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.50867187]\n",
      " [0.         0.         0.42068099 0.        ]\n",
      " [0.         0.4842629  0.33166972 0.        ]\n",
      " [0.         0.4842629  0.         0.40104275]\n",
      " [0.         0.         0.33166972 0.40104275]\n",
      " [0.55349232 0.         0.         0.40104275]\n",
      " [0.         0.         0.42068099 0.        ]\n",
      " [0.         0.61422608 0.         0.        ]]\n",
      "Rows:\n",
      "['and' 'build' 'exist' 'halloween' 'is' 'lantern' 'many' 'masks' 'people'\n",
      " 'pumpkins' 'scary' 'there' 'wear']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer() # parameters for tokenization, stopwords can be passed\n",
    "tfidf = vect.fit_transform([\"Halloween is scary!\",\n",
    "                            \"People wear halloween masks.\",\n",
    "                            \"There exist many masks and halloween pumpkins.\",\n",
    "                            \"People build scary lantern pumpkins.\"])\n",
    "\n",
    "print(\"TF-IDF vectors (each column is a document):\\n{}\\nRows:\\n{}\".format(tfidf.T.toarray(), vect.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing [cosine similarities](https://en.wikipedia.org/wiki/Cosine_similarity) for documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the documents: \n",
      "[[1.         0.17567864 0.1203216  0.22197408]\n",
      " [0.17567864 1.         0.26588742 0.19421012]\n",
      " [0.1203216  0.26588742 1.         0.13301374]\n",
      " [0.22197408 0.19421012 0.13301374 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cosine = (tfidf * tfidf.T).toarray()\n",
    "print(\"Cosine similarity between the documents: \\n{}\".format(cosine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing a new document (or searching) to the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New document:\n",
      "[[0.70203482]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.44809973]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.55349232]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "weights = vect.transform([\"Where to buy a halloween mask and pumpkins?\"])\n",
    "\n",
    "# HINT: If the text is completely different from the corpus, a zero vector will be returned\n",
    "# and therefore also not printed.\n",
    "print(\"New document:\\n{}\".format(weights.T.toarray())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try to calculate, which documents are the most similar to a query above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful data sources/tools\n",
    "\n",
    "When recognizing important parts of text, it is useful to help yourself with semantic databases or gazetteer lists (e.g., lists of organization names, countries, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet\n",
    "\n",
    "> WordNet is a lexical database for the English language. It groups English words into sets of synonyms called synsets, provides short definitions and usage examples, and records a number of relations among these synonym sets or their members. WordNet can thus be seen as a combination of dictionary and thesaurus. While it is accessible to human users via a web browser, its primary use is in automatic text analysis and artificial intelligence applications. The database and software tools have been released under a BSD style license and are freely available for download from the WordNet website. Both the lexicographic data (lexicographer files) and the compiler (called grind) for producing the distributed database are available.\n",
    "\n",
    "Let's find all sets of synonyms for word `book`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('book.n.01'),\n",
       " Synset('book.n.02'),\n",
       " Synset('record.n.05'),\n",
       " Synset('script.n.01'),\n",
       " Synset('ledger.n.01'),\n",
       " Synset('book.n.06'),\n",
       " Synset('book.n.07'),\n",
       " Synset('koran.n.01'),\n",
       " Synset('bible.n.01'),\n",
       " Synset('book.n.10'),\n",
       " Synset('book.n.11'),\n",
       " Synset('book.v.01'),\n",
       " Synset('reserve.v.04'),\n",
       " Synset('book.v.03'),\n",
       " Synset('book.v.04')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"book\")\n",
    "syns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out some data about these synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syset name: 'book.n.01'\n",
      "Lemmas:     '['book']'\n",
      "Definition: 'a written work or composition that has been published (printed on pages bound together)'\n",
      "Examples:   '['I am reading a good book on economics']'\n",
      "\n",
      "Syset name: 'book.n.02'\n",
      "Lemmas:     '['book', 'volume']'\n",
      "Definition: 'physical objects consisting of a number of pages bound together'\n",
      "Examples:   '['he used a large book as a doorstop']'\n",
      "\n",
      "Syset name: 'record.n.05'\n",
      "Lemmas:     '['record', 'record_book', 'book']'\n",
      "Definition: 'a compilation of the known facts regarding something or someone'\n",
      "Examples:   '[\"Al Smith used to say, `Let's look at the record'\", 'his name is in all the record books']'\n",
      "\n",
      "Syset name: 'script.n.01'\n",
      "Lemmas:     '['script', 'book', 'playscript']'\n",
      "Definition: 'a written version of a play or other dramatic composition; used in preparing for a performance'\n",
      "Examples:   '[]'\n",
      "\n",
      "Syset name: 'ledger.n.01'\n",
      "Lemmas:     '['ledger', 'leger', 'account_book', 'book_of_account', 'book']'\n",
      "Definition: 'a record in which commercial accounts are recorded'\n",
      "Examples:   '['they got a subpoena to examine our books']'\n",
      "\n",
      "Syset name: 'book.n.06'\n",
      "Lemmas:     '['book']'\n",
      "Definition: 'a collection of playing cards satisfying the rules of a card game'\n",
      "Examples:   '[]'\n",
      "\n",
      "Syset name: 'book.n.07'\n",
      "Lemmas:     '['book', 'rule_book']'\n",
      "Definition: 'a collection of rules or prescribed standards on the basis of which decisions are made'\n",
      "Examples:   '['they run things by the book around here']'\n",
      "\n",
      "Syset name: 'koran.n.01'\n",
      "Lemmas:     '['Koran', 'Quran', \"al-Qur'an\", 'Book']'\n",
      "Definition: 'the sacred writings of Islam revealed by God to the prophet Muhammad during his life at Mecca and Medina'\n",
      "Examples:   '[]'\n",
      "\n",
      "Syset name: 'bible.n.01'\n",
      "Lemmas:     '['Bible', 'Christian_Bible', 'Book', 'Good_Book', 'Holy_Scripture', 'Holy_Writ', 'Scripture', 'Word_of_God', 'Word']'\n",
      "Definition: 'the sacred writings of the Christian religions'\n",
      "Examples:   '['he went to carry the Word to the heathen']'\n",
      "\n",
      "Syset name: 'book.n.10'\n",
      "Lemmas:     '['book']'\n",
      "Definition: 'a major division of a long written composition'\n",
      "Examples:   '['the book of Isaiah']'\n",
      "\n",
      "Syset name: 'book.n.11'\n",
      "Lemmas:     '['book']'\n",
      "Definition: 'a number of sheets (ticket or stamps etc.) bound together on one edge'\n",
      "Examples:   '['he bought a book of stamps']'\n",
      "\n",
      "Syset name: 'book.v.01'\n",
      "Lemmas:     '['book']'\n",
      "Definition: 'engage for a performance'\n",
      "Examples:   '['Her agent had booked her for several concerts in Tokyo']'\n",
      "\n",
      "Syset name: 'reserve.v.04'\n",
      "Lemmas:     '['reserve', 'hold', 'book']'\n",
      "Definition: 'arrange for and reserve (something for someone else) in advance'\n",
      "Examples:   '['reserve me a seat on a flight', 'The agent booked tickets to the show for the whole family', \"please hold a table at Maxim's\"]'\n",
      "\n",
      "Syset name: 'book.v.03'\n",
      "Lemmas:     '['book']'\n",
      "Definition: 'record a charge in a police register'\n",
      "Examples:   '['The policeman booked her when she tried to solicit a man']'\n",
      "\n",
      "Syset name: 'book.v.04'\n",
      "Lemmas:     '['book']'\n",
      "Definition: 'register in a hotel booker'\n",
      "Examples:   '[]'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for synset in syns:\n",
    "    print(\"Syset name: '{}'\".format(synset.name()))\n",
    "    print(\"Lemmas:     '{}'\".format([lemma.name() for lemma in synset.lemmas()]))\n",
    "    print(\"Definition: '{}'\".format(synset.definition()))\n",
    "    print(\"Examples:   '{}'\\n\".format(synset.examples()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find synonyms, antonyms, hypernyms and hyponyms of a word `good`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms:  '{'estimable', 'goodness', 'unspoiled', 'in_effect', 'full', 'near', 'respectable', 'good', 'thoroughly', 'in_force', 'dear', 'adept', 'well', 'sound', 'right', 'just', 'upright', 'effective', 'expert', 'soundly', 'salutary', 'dependable', 'skilful', 'skillful', 'ripe', 'trade_good', 'proficient', 'honorable', 'safe', 'serious', 'undecomposed', 'honest', 'secure', 'practiced', 'unspoilt', 'beneficial', 'commodity'}'\n",
      "\n",
      "Antonyms:  '{'badness', 'bad', 'evil', 'evilness', 'ill'}'\n",
      "Hypernyms: '{'advantage.n.01', 'quality.n.01', 'artifact.n.01', 'morality.n.01'}'\n",
      "Hyponyms:  '{'optimum.n.01', 'benignity.n.01', 'export.n.01', 'common_good.n.01'}'\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "hypernyms = []\n",
    "hyponyms = []\n",
    "\n",
    "for synset in wordnet.synsets(\"good\"):\n",
    "    for lemma in synset.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "        if lemma.antonyms():\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "        if synset.hypernyms():\n",
    "            hypernyms.append(synset.hypernyms()[0].name())\n",
    "        if synset.hyponyms():\n",
    "            hyponyms.append(synset.hyponyms()[0].name())\n",
    "\n",
    "print(\"Synonyms:  '{}'\\n\".format(set(synonyms)))\n",
    "print(\"Antonyms:  '{}'\".format(set(antonyms)))\n",
    "print(\"Hypernyms: '{}'\".format(set(hypernyms)))\n",
    "print(\"Hyponyms:  '{}'\".format(set(hyponyms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare similarity of two synsets.\n",
    "\n",
    "The Wu & Palmer measure calculates relatedness by considering the depths of the two synsets in the WordNet taxonomies, along with the depth of the LCS (Least Common Subsumer). The formula is score = 2*depth(lcs) / (depth(s1) + depth(s2)). This means that 0 < score <= 1. The score can never be zero because the depth of the LCS is never zero (the depth of the root of a taxonomy is one). The score is one if the two input synsets are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good vs. nice: 0.12\n",
      " Good vs. bad: 0.67\n"
     ]
    }
   ],
   "source": [
    "good = wordnet.synsets(\"good\")[0]\n",
    "nice = wordnet.synsets(\"nice\")[0]\n",
    "bad  = wordnet.synsets(\"bad\")[0]\n",
    "\n",
    "print(\"Good vs. nice: {:.2}\".format(good.wup_similarity(nice)))\n",
    "print(\" Good vs. bad: {:.2}\".format(good.wup_similarity(bad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other sources\n",
    "\n",
    "A plethora of other data sources exist, for example:\n",
    "\n",
    "* [VerbNet](http://verbs.colorado.edu/~mpalmer/projects/verbnet.html)\n",
    "* [FrameNet](https://framenet.icsi.berkeley.edu/fndrupal/)\n",
    "* [SentiStrength](http://sentistrength.wlv.ac.uk/)\n",
    "* [BabelNet](http://babelnet.org/)\n",
    "* [ConceptNet](http://www.conceptnet.io/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpora\n",
    "\n",
    "To train or test an NLP system, we need some tagged data to compare the systems to. Some datasets exist from challenges, for example:\n",
    "\n",
    "* [CoNLL](http://www.conll.org/previous-tasks)\n",
    "* [SemEval](https://en.wikipedia.org/wiki/SemEval)\n",
    "* BioNLP [2016](http://2016.bionlp-st.org/), [2013](http://2013.bionlp-st.org/), [2011](http://2011.bionlp-st.org/), [2009](http://www.nactem.ac.uk/tsujii/GENIA/SharedTask/)\n",
    "* [CHEMDNER](http://www.biocreative.org/tasks/biocreative-iv/chemdner/)\n",
    "* [ACE2004](https://catalog.ldc.upenn.edu/LDC2005T09)\n",
    "* [Enron email dataset](https://www.cs.cmu.edu/~./enron/)\n",
    "* [Citeseer](http://csxstatic.ist.psu.edu/about/data)\n",
    "* [Reuters](http://www.daviddlewis.com/resources/testcollections/reuters21578/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select one of the NLP datasets or extract your own and then:\n",
    "\n",
    "* Overview its structure.\n",
    "* Report on dataset features (number of tags, words, sentences, ...)\n",
    "* Use some existing data source and define some manual rules to tackle the dataset problem. Report on the accuracy of your method (percentage of correct labels) and compare to baseline or existing work.\n",
    "\n",
    "You can also visualize data using [Matplotlib](http://matplotlib.org/gallery.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
