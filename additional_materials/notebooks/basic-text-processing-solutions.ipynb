{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcfaf8f4-d357-419e-b3e8-8ed47620d84c",
   "metadata": {},
   "source": [
    "# Text Processing Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40fb243-6c93-46ef-8291-f93f8c33dff0",
   "metadata": {},
   "source": [
    "## Exercise 1: Text Analysis Basics\n",
    "**Objective:** Get comfortable with basic string operations and text manipulation.\n",
    "\n",
    "- Write a Python script to count the number of words in a given text.\n",
    "- Create a function that identifies and counts the frequency of each unique word in a text.\n",
    "- Develop a script to find and replace specific words in a text with another word of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "171e1ef9-d0f9-4de9-99b9-46644dc6bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "In the heart of an ancient forest, a mysterious library stood untouched by time. Its shelves were laden with books of every conceivable subject, from the arcane arts to the natural sciences. The air was thick with the scent of old paper and whispers of knowledge long forgotten. Scholars from distant lands would journey for months to study its tomes, delving into secrets that were as old as the forest itself.\n",
    "\n",
    "One day, a young wanderer stumbled upon the library. With eyes wide with wonder, she explored its vast halls, her fingers brushing against the spines of books that had not been touched in centuries. The library seemed to welcome her, its dimly lit corridors flickering to life as she passed. In this haven of knowledge, the wanderer found not just the answers to her questions, but also questions she had never thought to ask.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55cab28-2d2c-447e-95e4-6e5e2cbb9985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/azagar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/azagar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afdee02e-0729-4584-a175-311463c4610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "{'in': 3, 'the': 11, 'heart': 1, 'of': 6, 'an': 1, 'ancient': 1, 'forest,': 1, 'a': 2, 'mysterious': 1, 'library': 2, 'stood': 1, 'untouched': 1, 'by': 1, 'time.': 1, 'its': 4, 'shelves': 1, 'were': 2, 'laden': 1, 'with': 4, 'books': 2, 'every': 1, 'conceivable': 1, 'subject,': 1, 'from': 2, 'arcane': 1, 'arts': 1, 'to': 6, 'natural': 1, 'sciences.': 1, 'air': 1, 'was': 1, 'thick': 1, 'scent': 1, 'old': 2, 'paper': 1, 'and': 1, 'whispers': 1, 'knowledge': 1, 'long': 1, 'forgotten.': 1, 'scholars': 1, 'distant': 1, 'lands': 1, 'would': 1, 'journey': 1, 'for': 1, 'months': 1, 'study': 1, 'tomes,': 1, 'delving': 1, 'into': 1, 'secrets': 1, 'that': 2, 'as': 3, 'forest': 1, 'itself.': 1, 'one': 1, 'day,': 1, 'young': 1, 'wanderer': 2, 'stumbled': 1, 'upon': 1, 'library.': 1, 'eyes': 1, 'wide': 1, 'wonder,': 1, 'she': 3, 'explored': 1, 'vast': 1, 'halls,': 1, 'her': 2, 'fingers': 1, 'brushing': 1, 'against': 1, 'spines': 1, 'had': 2, 'not': 2, 'been': 1, 'touched': 1, 'centuries.': 1, 'seemed': 1, 'welcome': 1, 'her,': 1, 'dimly': 1, 'lit': 1, 'corridors': 1, 'flickering': 1, 'life': 1, 'passed.': 1, 'this': 1, 'haven': 1, 'knowledge,': 1, 'found': 1, 'just': 1, 'answers': 1, 'questions,': 1, 'but': 1, 'also': 1, 'questions': 1, 'never': 1, 'thought': 1, 'ask.': 1}\n",
      "\n",
      "In the heart of an ancient forest, a mysterious sanctuary stood untouched by time. Its shelves were laden with books of every conceivable subject, from the arcane arts to the natural sciences. The air was thick with the scent of old paper and whispers of knowledge long forgotten. Scholars from distant lands would journey for months to study its tomes, delving into secrets that were as old as the forest itself.\n",
      "\n",
      "One day, a young wanderer stumbled upon the sanctuary. With eyes wide with wonder, she explored its vast halls, her fingers brushing against the spines of books that had not been touched in centuries. The sanctuary seemed to welcome her, its dimly lit corridors flickering to life as she passed. In this haven of knowledge, the wanderer found not just the answers to her questions, but also questions she had never thought to ask.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.1: Count the number of words in the given text\n",
    "def count_words(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "# Exercise 1.2: Identify and count the frequency of each unique word\n",
    "def word_frequencies(text):\n",
    "    words = text.lower().split()\n",
    "    frequencies = {}\n",
    "    for word in words:\n",
    "        if word in frequencies:\n",
    "            frequencies[word] += 1\n",
    "        else:\n",
    "            frequencies[word] = 1\n",
    "    return frequencies\n",
    "\n",
    "# Exercise 1.3: Find and replace specific words\n",
    "def find_and_replace(text, old_word, new_word):\n",
    "    replaced_text = text.replace(old_word, new_word)\n",
    "    return replaced_text\n",
    "\n",
    "# Solutions\n",
    "word_count = count_words(sample_text)\n",
    "print(word_count)\n",
    "frequencies = word_frequencies(sample_text)\n",
    "print(frequencies)\n",
    "replaced_text = find_and_replace(sample_text, \"library\", \"sanctuary\")\n",
    "print(replaced_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0062f298-62db-4d46-83a6-cf0def71a7fd",
   "metadata": {},
   "source": [
    "## Exercise 2: Regular Expressions\n",
    "**Objective:** Practice using regular expressions for pattern matching and text manipulation.\n",
    "\n",
    "- Write a Python function that uses regular expressions to find all email addresses in a given text.\n",
    "- Create a script that extracts all dates (in the format xx/xx/xxxx) from a text.\n",
    "- Develop a regular expression that identifies all occurrences of Slovene phone numbers in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad8c45c-b47d-4f71-a856-4ccea0f06f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['john.doe@example.com', 'jane_doe123@workmail.com', 'admin@ourwebsite.org'],\n",
       " ['15/08/1999'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the sample text\n",
    "sample_text = \"\"\"\n",
    "John's email is john.doe@example.com, and he started working with us on 3rd April 2021. For inquiries, you can also reach out to Jane at jane_doe123@workmail.com. Our office was established on 15/08/1999, and since then, we have been located at 123 Baker Street. Remember to mark the important date, 01-Jan-2023, for our annual meeting. For more information, visit our website or contact admin@ourwebsite.org.\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression for finding email addresses\n",
    "email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "\n",
    "# Regular expression for finding dates in the format xx/xx/xxxx\n",
    "date_regex = r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b'\n",
    "\n",
    "\n",
    "# Find all matches in the text\n",
    "email_addresses = re.findall(email_regex, sample_text)\n",
    "dates = re.findall(date_regex, sample_text)\n",
    "\n",
    "# Extracting the full match from the date tuples\n",
    "dates = [''.join(date) for date in dates]\n",
    "\n",
    "(email_addresses, dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27fcb4a-2db4-49da-b4f4-8b085ef13412",
   "metadata": {},
   "source": [
    "## Exercise 3: Text Preprocessing Techniques for Slovene\n",
    "**Objective:** Deepen understanding of text preprocessing techniques.\n",
    "\n",
    "- Use classla pipeline to process given text.\n",
    "- Iterate through text and print words, their lemmas and POS tags line by line. \n",
    "- Find a list of Slovene stopwords on the web and filter them out from the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73bfd43f-a625-4e50-b36b-ca501df7b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "slovene_text = \"\"\"\n",
    "V središču starega mesta Ljubljana stoji mogočna Ljubljanska katedrala, ki privablja obiskovalce iz vseh koncev sveta. Zgrajena v baročnem slogu, ta arhitekturni biser razkriva zgodovino in kulturo slovenske prestolnice. Njene veličastne freske in izdelano rezbarstvo vzbujajo občudovanje in spoštovanje med vsemi, ki prestopijo njen prag.\n",
    "\n",
    "Le nekaj ulic stran, ob bregovih reke Ljubljanice, se razteza živahna tržnica, kjer lokalni pridelovalci vsak dan ponujajo sveže sadje, zelenjavo in druge domače izdelke. Ta kraj je središče mestnega vrveža in priljubljeno zbirališče tako za domačine kot turiste. Sprehajalci lahko uživajo v prijetnem vzdušju, ki ga ustvarjajo številne kavarne in restavracije, ki obdajajo tržnico.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0d2e50a-4e58-4bcc-92fa-e9cf35744edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 11:46:16 INFO: Loading these models for language: sl (Slovenian):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | standard |\n",
      "| pos       | standard |\n",
      "| lemma     | standard |\n",
      "| depparse  | standard |\n",
      "| ner       | standard |\n",
      "========================\n",
      "\n",
      "2024-03-03 11:46:16 INFO: Use device: cpu\n",
      "2024-03-03 11:46:16 INFO: Loading: tokenize\n",
      "2024-03-03 11:46:16 INFO: Loading: pos\n",
      "2024-03-03 11:46:22 INFO: Loading: lemma\n",
      "2024-03-03 11:46:29 INFO: Loading: depparse\n",
      "2024-03-03 11:46:30 INFO: Loading: ner\n",
      "2024-03-03 11:46:30 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import classla\n",
    "\n",
    "# Initialize the Classla pipeline for Slovene\n",
    "nlp = classla.Pipeline('sl', processors='tokenize,ner,pos,lemma,depparse')                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ee5166-e28f-4c44-bf75-e6bf75bc4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text\n",
    "doc = nlp(slovene_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c59e8648-a3dc-403c-865c-d3d99d119a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: V\tLemma: v\tPart of Speech: ADP\n",
      "Word: središču\tLemma: središče\tPart of Speech: NOUN\n",
      "Word: starega\tLemma: star\tPart of Speech: ADJ\n",
      "Word: mesta\tLemma: mesto\tPart of Speech: NOUN\n",
      "Word: Ljubljana\tLemma: Ljubljana\tPart of Speech: PROPN\n",
      "Word: stoji\tLemma: stati\tPart of Speech: VERB\n",
      "Word: mogočna\tLemma: mogočen\tPart of Speech: ADJ\n",
      "Word: Ljubljanska\tLemma: ljubljanski\tPart of Speech: ADJ\n",
      "Word: katedrala\tLemma: katedrala\tPart of Speech: NOUN\n",
      "Word: ,\tLemma: ,\tPart of Speech: PUNCT\n",
      "Word: ki\tLemma: ki\tPart of Speech: SCONJ\n",
      "Word: privablja\tLemma: privabljati\tPart of Speech: VERB\n",
      "Word: obiskovalce\tLemma: obiskovalec\tPart of Speech: NOUN\n",
      "Word: iz\tLemma: iz\tPart of Speech: ADP\n",
      "Word: vseh\tLemma: ves\tPart of Speech: DET\n",
      "Word: koncev\tLemma: konec\tPart of Speech: NOUN\n",
      "Word: sveta\tLemma: svet\tPart of Speech: NOUN\n",
      "Word: .\tLemma: .\tPart of Speech: PUNCT\n",
      "Word: Zgrajena\tLemma: zgrajen\tPart of Speech: ADJ\n",
      "Word: v\tLemma: v\tPart of Speech: ADP\n",
      "Word: baročnem\tLemma: baročen\tPart of Speech: ADJ\n",
      "Word: slogu\tLemma: slog\tPart of Speech: NOUN\n",
      "Word: ,\tLemma: ,\tPart of Speech: PUNCT\n",
      "Word: ta\tLemma: ta\tPart of Speech: DET\n",
      "Word: arhitekturni\tLemma: arhitekturen\tPart of Speech: ADJ\n",
      "Word: biser\tLemma: biser\tPart of Speech: NOUN\n",
      "Word: razkriva\tLemma: razkrivati\tPart of Speech: VERB\n",
      "Word: zgodovino\tLemma: zgodovina\tPart of Speech: NOUN\n",
      "Word: in\tLemma: in\tPart of Speech: CCONJ\n",
      "Word: kulturo\tLemma: kultura\tPart of Speech: NOUN\n",
      "Word: slovenske\tLemma: slovenski\tPart of Speech: ADJ\n",
      "Word: prestolnice\tLemma: prestolnica\tPart of Speech: NOUN\n",
      "Word: .\tLemma: .\tPart of Speech: PUNCT\n",
      "Word: Njene\tLemma: njen\tPart of Speech: DET\n",
      "Word: veličastne\tLemma: veličasten\tPart of Speech: ADJ\n",
      "Word: freske\tLemma: freska\tPart of Speech: NOUN\n",
      "Word: in\tLemma: in\tPart of Speech: CCONJ\n",
      "Word: izdelano\tLemma: izdelan\tPart of Speech: ADJ\n",
      "Word: rezbarstvo\tLemma: rezbarstvo\tPart of Speech: NOUN\n",
      "Word: vzbujajo\tLemma: vzbujati\tPart of Speech: VERB\n",
      "Word: občudovanje\tLemma: občudovanje\tPart of Speech: NOUN\n",
      "Word: in\tLemma: in\tPart of Speech: CCONJ\n",
      "Word: spoštovanje\tLemma: spoštovanje\tPart of Speech: NOUN\n",
      "Word: med\tLemma: med\tPart of Speech: ADP\n",
      "Word: vsemi\tLemma: ves\tPart of Speech: DET\n",
      "Word: ,\tLemma: ,\tPart of Speech: PUNCT\n",
      "Word: ki\tLemma: ki\tPart of Speech: SCONJ\n",
      "Word: prestopijo\tLemma: prestopiti\tPart of Speech: VERB\n",
      "Word: njen\tLemma: njen\tPart of Speech: DET\n",
      "Word: prag\tLemma: prag\tPart of Speech: NOUN\n",
      "Word: .\tLemma: .\tPart of Speech: PUNCT\n",
      "Word: Le\tLemma: le\tPart of Speech: PART\n",
      "Word: nekaj\tLemma: nekaj\tPart of Speech: DET\n",
      "Word: ulic\tLemma: ulica\tPart of Speech: NOUN\n",
      "Word: stran\tLemma: stran\tPart of Speech: NOUN\n",
      "Word: ,\tLemma: ,\tPart of Speech: PUNCT\n",
      "Word: ob\tLemma: ob\tPart of Speech: ADP\n",
      "Word: bregovih\tLemma: breg\tPart of Speech: NOUN\n",
      "Word: reke\tLemma: reka\tPart of Speech: NOUN\n",
      "Word: Ljubljanice\tLemma: Ljubljanica\tPart of Speech: PROPN\n",
      "Word: ,\tLemma: ,\tPart of Speech: PUNCT\n",
      "Word: se\tLemma: se\tPart of Speech: PRON\n",
      "Word: razteza\tLemma: raztezati\tPart of Speech: VERB\n",
      "Word: živahna\tLemma: živahen\tPart of Speech: ADJ\n",
      "Word: tržnica\tLemma: tržnica\tPart of Speech: NOUN\n",
      "Word: ,\tLemma: ,\tPart of Speech: PUNCT\n",
      "Word: kjer\tLemma: kjer\tPart of Speech: SCONJ\n",
      "Word: lokalni\tLemma: lokalen\tPart of Speech: ADJ\n",
      "Word: pridelovalci\tLemma: pridelovalec\tPart of Speech: NOUN\n",
      "Word: vsak\tLemma: vsak\tPart of Speech: DET\n",
      "Word: dan\tLemma: dan\tPart of Speech: NOUN\n",
      "Word: ponujajo\tLemma: ponujati\tPart of Speech: VERB\n",
      "Word: sveže\tLemma: svež\tPart of Speech: ADJ\n",
      "Word: sadje\tLemma: sadje\tPart of Speech: NOUN\n",
      "Word: ,\tLemma: ,\tPart of Speech: PUNCT\n",
      "Word: zelenjavo\tLemma: zelenjava\tPart of Speech: NOUN\n",
      "Word: in\tLemma: in\tPart of Speech: CCONJ\n",
      "Word: druge\tLemma: drug\tPart of Speech: ADJ\n",
      "Word: domače\tLemma: domač\tPart of Speech: ADJ\n",
      "Word: izdelke\tLemma: izdelek\tPart of Speech: NOUN\n",
      "Word: .\tLemma: .\tPart of Speech: PUNCT\n",
      "Word: Ta\tLemma: ta\tPart of Speech: DET\n",
      "Word: kraj\tLemma: kraj\tPart of Speech: NOUN\n",
      "Word: je\tLemma: biti\tPart of Speech: AUX\n",
      "Word: središče\tLemma: središče\tPart of Speech: NOUN\n",
      "Word: mestnega\tLemma: mesten\tPart of Speech: ADJ\n",
      "Word: vrveža\tLemma: vrvež\tPart of Speech: NOUN\n",
      "Word: in\tLemma: in\tPart of Speech: CCONJ\n",
      "Word: priljubljeno\tLemma: priljubljen\tPart of Speech: ADJ\n",
      "Word: zbirališče\tLemma: zbirališče\tPart of Speech: NOUN\n",
      "Word: tako\tLemma: tako\tPart of Speech: ADV\n",
      "Word: za\tLemma: za\tPart of Speech: ADP\n",
      "Word: domačine\tLemma: domačin\tPart of Speech: NOUN\n",
      "Word: kot\tLemma: kot\tPart of Speech: SCONJ\n",
      "Word: turiste\tLemma: turist\tPart of Speech: NOUN\n",
      "Word: .\tLemma: .\tPart of Speech: PUNCT\n",
      "Word: Sprehajalci\tLemma: sprehajalec\tPart of Speech: NOUN\n",
      "Word: lahko\tLemma: lahko\tPart of Speech: ADV\n",
      "Word: uživajo\tLemma: uživati\tPart of Speech: VERB\n",
      "Word: v\tLemma: v\tPart of Speech: ADP\n",
      "Word: prijetnem\tLemma: prijeten\tPart of Speech: ADJ\n",
      "Word: vzdušju\tLemma: vzdušje\tPart of Speech: NOUN\n",
      "Word: ,\tLemma: ,\tPart of Speech: PUNCT\n",
      "Word: ki\tLemma: ki\tPart of Speech: SCONJ\n",
      "Word: ga\tLemma: on\tPart of Speech: PRON\n",
      "Word: ustvarjajo\tLemma: ustvarjati\tPart of Speech: VERB\n",
      "Word: številne\tLemma: številen\tPart of Speech: ADJ\n",
      "Word: kavarne\tLemma: kavarna\tPart of Speech: NOUN\n",
      "Word: in\tLemma: in\tPart of Speech: CCONJ\n",
      "Word: restavracije\tLemma: restavracija\tPart of Speech: NOUN\n",
      "Word: ,\tLemma: ,\tPart of Speech: PUNCT\n",
      "Word: ki\tLemma: ki\tPart of Speech: SCONJ\n",
      "Word: obdajajo\tLemma: obdajati\tPart of Speech: VERB\n",
      "Word: tržnico\tLemma: tržnica\tPart of Speech: NOUN\n",
      "Word: .\tLemma: .\tPart of Speech: PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Iterate through sentences and tokens to extract information\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(f\"Word: {word.text}\\tLemma: {word.lemma}\\tPart of Speech: {word.upos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eb7855b-ec6e-4bd5-9449-0118c8766c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slovene Stopwords:\n",
      "['a', 'ali', 'april', 'avgust', 'b', 'bi', 'bil', 'bila', 'bile', 'bili', 'bilo', 'biti', 'blizu', 'bo', 'bodo', 'bojo', 'bolj', 'bom', 'bomo', 'boste', 'bova', 'boš', 'brez', 'c', 'cel', 'cela', 'celi', 'celo', 'd', 'da', 'daleč', 'dan', 'danes', 'datum', 'december', 'deset', 'deseta', 'deseti', 'deseto', 'devet', 'deveta', 'deveti', 'deveto', 'do', 'dober', 'dobra', 'dobri', 'dobro', 'dokler', 'dol', 'dolg', 'dolga', 'dolgi', 'dovolj', 'drug', 'druga', 'drugi', 'drugo', 'dva', 'dve', 'e', 'eden', 'en', 'ena', 'ene', 'eni', 'enkrat', 'eno', 'etc.', 'f', 'februar', 'g', 'g.', 'ga', 'ga.', 'gor', 'gospa', 'gospod', 'h', 'halo', 'i', 'idr.', 'ii', 'iii', 'in', 'iv', 'ix', 'iz', 'j', 'januar', 'jaz', 'je', 'ji', 'jih', 'jim', 'jo', 'julij', 'junij', 'jutri', 'k', 'kadarkoli', 'kaj', 'kajti', 'kako', 'kakor', 'kamor', 'kamorkoli', 'kar', 'karkoli', 'katerikoli', 'kdaj', 'kdo', 'kdorkoli', 'ker', 'ki', 'kje', 'kjer', 'kjerkoli', 'ko', 'koder', 'koderkoli', 'koga', 'komu', 'kot', 'kratek', 'kratka', 'kratke', 'kratki', 'l', 'lahka', 'lahke', 'lahki', 'lahko', 'le', 'lep', 'lepa', 'lepe', 'lepi', 'lepo', 'leto', 'm', 'maj', 'majhen', 'majhna', 'majhni', 'malce', 'malo', 'manj', 'marec', 'me', 'med', 'medtem', 'mene', 'mesec', 'mi', 'midva', 'midve', 'mnogo', 'moj', 'moja', 'moje', 'mora', 'morajo', 'moram', 'moramo', 'morate', 'moraš', 'morem', 'mu', 'n', 'na', 'nad', 'naj', 'najina', 'najino', 'najmanj', 'naju', 'največ', 'nam', 'narobe', 'nas', 'nato', 'nazaj', 'naš', 'naša', 'naše', 'ne', 'nedavno', 'nedelja', 'nek', 'neka', 'nekaj', 'nekatere', 'nekateri', 'nekatero', 'nekdo', 'neke', 'nekega', 'neki', 'nekje', 'neko', 'nekoga', 'nekoč', 'ni', 'nikamor', 'nikdar', 'nikjer', 'nikoli', 'nič', 'nje', 'njega', 'njegov', 'njegova', 'njegovo', 'njej', 'njemu', 'njen', 'njena', 'njeno', 'nji', 'njih', 'njihov', 'njihova', 'njihovo', 'njiju', 'njim', 'njo', 'njun', 'njuna', 'njuno', 'no', 'nocoj', 'november', 'npr.', 'o', 'ob', 'oba', 'obe', 'oboje', 'od', 'odprt', 'odprta', 'odprti', 'okoli', 'oktober', 'on', 'onadva', 'one', 'oni', 'onidve', 'osem', 'osma', 'osmi', 'osmo', 'oz.', 'p', 'pa', 'pet', 'peta', 'petek', 'peti', 'peto', 'po', 'pod', 'pogosto', 'poleg', 'poln', 'polna', 'polni', 'polno', 'ponavadi', 'ponedeljek', 'ponovno', 'potem', 'povsod', 'pozdravljen', 'pozdravljeni', 'prav', 'prava', 'prave', 'pravi', 'pravo', 'prazen', 'prazna', 'prazno', 'prbl.', 'precej', 'pred', 'prej', 'preko', 'pri', 'pribl.', 'približno', 'primer', 'pripravljen', 'pripravljena', 'pripravljeni', 'proti', 'prva', 'prvi', 'prvo', 'r', 'ravno', 'redko', 'res', 'reč', 's', 'saj', 'sam', 'sama', 'same', 'sami', 'samo', 'se', 'sebe', 'sebi', 'sedaj', 'sedem', 'sedma', 'sedmi', 'sedmo', 'sem', 'september', 'seveda', 'si', 'sicer', 'skoraj', 'skozi', 'slab', 'smo', 'so', 'sobota', 'spet', 'sreda', 'srednja', 'srednji', 'sta', 'ste', 'stran', 'stvar', 'sva', 't', 'ta', 'tak', 'taka', 'take', 'taki', 'tako', 'takoj', 'tam', 'te', 'tebe', 'tebi', 'tega', 'težak', 'težka', 'težki', 'težko', 'ti', 'tista', 'tiste', 'tisti', 'tisto', 'tj.', 'tja', 'to', 'toda', 'torek', 'tretja', 'tretje', 'tretji', 'tri', 'tu', 'tudi', 'tukaj', 'tvoj', 'tvoja', 'tvoje', 'u', 'v', 'vaju', 'vam', 'vas', 'vaš', 'vaša', 'vaše', 've', 'vedno', 'velik', 'velika', 'veliki', 'veliko', 'vendar', 'ves', 'več', 'vi', 'vidva', 'vii', 'viii', 'visok', 'visoka', 'visoke', 'visoki', 'vsa', 'vsaj', 'vsak', 'vsaka', 'vsakdo', 'vsake', 'vsaki', 'vsakomur', 'vse', 'vsega', 'vsi', 'vso', 'včasih', 'včeraj', 'x', 'z', 'za', 'zadaj', 'zadnji', 'zakaj', 'zaprta', 'zaprti', 'zaprto', 'zdaj', 'zelo', 'zunaj', 'č', 'če', 'često', 'četrta', 'četrtek', 'četrti', 'četrto', 'čez', 'čigav', 'š', 'šest', 'šesta', 'šesti', 'šesto', 'štiri', 'ž', 'že']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the Slovene stopwords list\n",
    "url = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-sl/master/stopwords-sl.txt\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Decode the content and split by new lines to get a list of stopwords\n",
    "    stopwords = response.content.decode('utf-8').splitlines()\n",
    "    print(\"Slovene Stopwords:\")\n",
    "    print(stopwords)\n",
    "else:\n",
    "    print(f\"Failed to fetch stopwords. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2754c8a5-739d-4c1c-8537-a2be3780176f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized and Filtered Tokens:\n",
      "['središču', 'starega', 'mesta', 'Ljubljana', 'stoji', 'mogočna', 'Ljubljanska', 'katedrala', ',', 'privablja', 'obiskovalce', 'koncev', 'sveta', '.', 'Zgrajena', 'baročnem', 'slogu', ',', 'arhitekturni', 'biser', 'razkriva', 'zgodovino', 'kulturo', 'slovenske', 'prestolnice', '.', 'veličastne', 'freske', 'izdelano', 'rezbarstvo', 'vzbujajo', 'občudovanje', 'spoštovanje', ',', 'prestopijo', 'prag', '.', 'ulic', ',', 'bregovih', 'reke', 'Ljubljanice', ',', 'razteza', 'živahna', 'tržnica', ',', 'lokalni', 'pridelovalci', 'ponujajo', 'sveže', 'sadje', ',', 'zelenjavo', 'domače', 'izdelke', '.', 'kraj', 'središče', 'mestnega', 'vrveža', 'priljubljeno', 'zbirališče', 'domačine', 'turiste', '.', 'Sprehajalci', 'uživajo', 'prijetnem', 'vzdušju', ',', 'ustvarjajo', 'številne', 'kavarne', 'restavracije', ',', 'obdajajo', 'tržnico', '.']\n"
     ]
    }
   ],
   "source": [
    "# Perform Lemmatization and Filter Out Stopwords\n",
    "lemmatized_filtered_tokens = []\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        if word.lemma.lower() not in stopwords:  \n",
    "            lemmatized_filtered_tokens.append(word.text)\n",
    "\n",
    "print(\"Lemmatized and Filtered Tokens:\")\n",
    "print(lemmatized_filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18b7f2-d4ba-4517-afdc-507d9e20cf46",
   "metadata": {},
   "source": [
    "## Exercise 4: Rule-Based Systems\n",
    "**Objective:** Understand and apply rule-based systems for text processing.\n",
    "\n",
    "- Develop a script that can extract named entities (like names of people, places, etc.) from a list of messages using rule-based patterns.\n",
    "- Design a simple rule-based system that can classify text messages as \"spam\" or \"not spam\" based on specific keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "875af3b6-108e-40a9-9e3d-90c84e675e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data: List of text messages\n",
    "messages = [\n",
    "    \"Win a FREE iPhone! Click here to claim now!\",\n",
    "    \"Dear John, your subscription to 'Tech Today' has been confirmed.\",\n",
    "    \"You have won $1000 in the Global Lottery! Send your bank details to claim.\",\n",
    "    \"Reminder: Meeting with the marketing team at 10 AM in the Tesla Conference Room.\",\n",
    "    \"This is your final reminder to pay your Verizon phone bill.\",\n",
    "    \"Congratulations, Sarah! You've been selected for a chance to win a Bahamas cruise!\",\n",
    "    \"Exclusive offer for Amazon Prime members: Unlock 50% discount on your next purchase.\",\n",
    "    \"Your FedEx package has been shipped and is on its way to 123 Elm Street!\",\n",
    "    \"Reminder: Your dental appointment with Dr. Anderson is scheduled for tomorrow at 3 PM.\",\n",
    "    \"Claim your FREE trial of Adobe Photoshop today.\",\n",
    "    \"Urgent: Your Chase Bank account has been compromised! Change your password immediately.\",\n",
    "    \"Join Microsoft's webinar on the future of artificial intelligence.\",\n",
    "    \"Get rid of debt now! Consolidate your loans with Goldman Sachs into one low monthly payment.\",\n",
    "    \"Happy Birthday, Emily! Enjoy a complimentary dinner at Olive Garden.\",\n",
    "    \"Your Netflix membership renewal is due. Please update your billing information.\",\n",
    "    \"You're invited to Google's exclusive networking event this Friday in San Francisco.\",\n",
    "    \"Act now to extend your Toyota car warranty at a special discounted rate.\",\n",
    "    \"Final notice: Your eBay account will be deactivated unless action is taken.\",\n",
    "    \"Congratulations, Dave! You've earned a reward from Starbucks! Click to redeem your points.\",\n",
    "    \"Survey invitation from Airbnb: Share your feedback and receive a $10 gift card.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c083481-73a8-411d-bda3-186cd8e18811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: 'Win a FREE iPhone! Click here to claim now!'\n",
      "Classification: Spam\n",
      "\n",
      "Message: 'Dear John, your subscription to 'Tech Today' has been confirmed.'\n",
      "Classification: Not Spam\n",
      "\n",
      "Message: 'You have won $1000 in the Global Lottery! Send your bank details to claim.'\n",
      "Classification: Spam\n",
      "\n",
      "Message: 'Reminder: Meeting with the marketing team at 10 AM in the Tesla Conference Room.'\n",
      "Classification: Not Spam\n",
      "\n",
      "Message: 'This is your final reminder to pay your Verizon phone bill.'\n",
      "Classification: Not Spam\n",
      "\n",
      "Message: 'Congratulations, Sarah! You've been selected for a chance to win a Bahamas cruise!'\n",
      "Classification: Spam\n",
      "\n",
      "Message: 'Exclusive offer for Amazon Prime members: Unlock 50% discount on your next purchase.'\n",
      "Classification: Not Spam\n",
      "\n",
      "Message: 'Your FedEx package has been shipped and is on its way to 123 Elm Street!'\n",
      "Classification: Not Spam\n",
      "\n",
      "Message: 'Reminder: Your dental appointment with Dr. Anderson is scheduled for tomorrow at 3 PM.'\n",
      "Classification: Not Spam\n",
      "\n",
      "Message: 'Claim your FREE trial of Adobe Photoshop today.'\n",
      "Classification: Spam\n",
      "\n",
      "Message: 'Urgent: Your Chase Bank account has been compromised! Change your password immediately.'\n",
      "Classification: Not Spam\n",
      "\n",
      "Message: 'Join Microsoft's webinar on the future of artificial intelligence.'\n",
      "Classification: Not Spam\n",
      "\n",
      "Message: 'Get rid of debt now! Consolidate your loans with Goldman Sachs into one low monthly payment.'\n",
      "Classification: Not Spam\n",
      "\n",
      "Message: 'Happy Birthday, Emily! Enjoy a complimentary dinner at Olive Garden.'\n",
      "Classification: Not Spam\n",
      "\n",
      "Message: 'Your Netflix membership renewal is due. Please update your billing information.'\n",
      "Classification: Not Spam\n",
      "\n",
      "Message: 'You're invited to Google's exclusive networking event this Friday in San Francisco.'\n",
      "Classification: Not Spam\n",
      "\n",
      "Message: 'Act now to extend your Toyota car warranty at a special discounted rate.'\n",
      "Classification: Not Spam\n",
      "\n",
      "Message: 'Final notice: Your eBay account will be deactivated unless action is taken.'\n",
      "Classification: Not Spam\n",
      "\n",
      "Message: 'Congratulations, Dave! You've earned a reward from Starbucks! Click to redeem your points.'\n",
      "Classification: Spam\n",
      "\n",
      "Message: 'Survey invitation from Airbnb: Share your feedback and receive a $10 gift card.'\n",
      "Classification: Not Spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a list of spammy keywords\n",
    "spam_keywords = ['win', 'free', 'claim', 'congratulations', 'lottery', 'click here']\n",
    "\n",
    "# Function to classify messages\n",
    "def classify_messages(messages, spam_keywords):\n",
    "    classifications = []\n",
    "    for message in messages:\n",
    "        # Convert message to lowercase and check for spam keywords\n",
    "        if any(spam_keyword in message.lower() for spam_keyword in spam_keywords):\n",
    "            classifications.append(\"Spam\")\n",
    "        else:\n",
    "            classifications.append(\"Not Spam\")\n",
    "    return classifications\n",
    "\n",
    "# Classify the messages\n",
    "classifications = classify_messages(messages, spam_keywords)\n",
    "\n",
    "# Print results\n",
    "for message, classification in zip(messages, classifications):\n",
    "    print(f\"Message: '{message}'\\nClassification: {classification}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5682071-d111-4fdb-9f4c-93adfdffd97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: 'Win a FREE iPhone! Click here to claim now!'\n",
      "Extracted Entities: ['FREE', 'Click']\n",
      "\n",
      "Message: 'Dear John, your subscription to 'Tech Today' has been confirmed.'\n",
      "Extracted Entities: ['Tech']\n",
      "\n",
      "Message: 'You have won $1000 in the Global Lottery! Send your bank details to claim.'\n",
      "Extracted Entities: ['Global', 'Send']\n",
      "\n",
      "Message: 'Reminder: Meeting with the marketing team at 10 AM in the Tesla Conference Room.'\n",
      "Extracted Entities: ['Meeting', 'AM', 'Tesla Conference']\n",
      "\n",
      "Message: 'This is your final reminder to pay your Verizon phone bill.'\n",
      "Extracted Entities: ['Verizon']\n",
      "\n",
      "Message: 'Congratulations, Sarah! You've been selected for a chance to win a Bahamas cruise!'\n",
      "Extracted Entities: ['Bahamas']\n",
      "\n",
      "Message: 'Exclusive offer for Amazon Prime members: Unlock 50% discount on your next purchase.'\n",
      "Extracted Entities: ['Amazon Prime', 'Unlock']\n",
      "\n",
      "Message: 'Your FedEx package has been shipped and is on its way to 123 Elm Street!'\n",
      "Extracted Entities: ['FedEx', 'Elm']\n",
      "\n",
      "Message: 'Reminder: Your dental appointment with Dr. Anderson is scheduled for tomorrow at 3 PM.'\n",
      "Extracted Entities: ['Your', 'Dr. Anderson']\n",
      "\n",
      "Message: 'Claim your FREE trial of Adobe Photoshop today.'\n",
      "Extracted Entities: ['FREE', 'Adobe Photoshop']\n",
      "\n",
      "Message: 'Urgent: Your Chase Bank account has been compromised! Change your password immediately.'\n",
      "Extracted Entities: ['Your Chase Bank', 'Change']\n",
      "\n",
      "Message: 'Join Microsoft's webinar on the future of artificial intelligence.'\n",
      "Extracted Entities: []\n",
      "\n",
      "Message: 'Get rid of debt now! Consolidate your loans with Goldman Sachs into one low monthly payment.'\n",
      "Extracted Entities: ['Consolidate', 'Goldman Sachs']\n",
      "\n",
      "Message: 'Happy Birthday, Emily! Enjoy a complimentary dinner at Olive Garden.'\n",
      "Extracted Entities: ['Enjoy', 'Olive']\n",
      "\n",
      "Message: 'Your Netflix membership renewal is due. Please update your billing information.'\n",
      "Extracted Entities: ['Netflix']\n",
      "\n",
      "Message: 'You're invited to Google's exclusive networking event this Friday in San Francisco.'\n",
      "Extracted Entities: ['Friday', 'San']\n",
      "\n",
      "Message: 'Act now to extend your Toyota car warranty at a special discounted rate.'\n",
      "Extracted Entities: ['Toyota']\n",
      "\n",
      "Message: 'Final notice: Your eBay account will be deactivated unless action is taken.'\n",
      "Extracted Entities: ['Your', 'Bay']\n",
      "\n",
      "Message: 'Congratulations, Dave! You've earned a reward from Starbucks! Click to redeem your points.'\n",
      "Extracted Entities: ['Click']\n",
      "\n",
      "Message: 'Survey invitation from Airbnb: Share your feedback and receive a $10 gift card.'\n",
      "Extracted Entities: ['Share']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_named_entities(message):\n",
    "    # Pattern to match sequences of capitalized words, possibly including single-letter capitalized words (initials) and abbreviations\n",
    "    # This pattern allows for a space or a period (for initials and abbreviations) followed by a capitalized word\n",
    "    # The lookahead assertion (?=\\s) ensures that the match is followed by a space, aiming to exclude possessive cases and contractions\n",
    "    entity_pattern = r'(?<!^)(?<!\\.\\s)(?:[A-Z][a-z]*\\.?\\s?)+(?=\\s)'\n",
    "\n",
    "    # Find all matches in the message\n",
    "    potential_entities = re.findall(entity_pattern, message)\n",
    "\n",
    "    # Post-processing to trim any trailing spaces or periods from the matches\n",
    "    potential_entities = [entity.strip('. ') for entity in potential_entities]\n",
    "\n",
    "    return potential_entities\n",
    "\n",
    "# Apply the extraction function to each message\n",
    "for message in messages:\n",
    "    extracted_entities = extract_named_entities(message)\n",
    "    print(f\"Message: '{message}'\\nExtracted Entities: {extracted_entities}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb96d01d-fdaa-43d4-a8d5-0d50d5e11d53",
   "metadata": {},
   "source": [
    "## Exercise 5: Corpus Analysis\n",
    "**Objective:** Gain experience in working with and analyzing text corpora.\n",
    "\n",
    "- Download a text corpus in Slovene (ccKres): https://www.clarin.si/repository/xmlui/handle/11356/1034.\n",
    "- Text format of the corpus contains a lot of documents. Sample and store 1000 of them.\n",
    "- Analyze the corpus for collocations (frequent word pairs or triplets) and report your findings. NOTE: Since the dataset is relatively large, you can use .split() method instead of classla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e8a1359-f1e8-4ef8-a50a-d7b0cbefe7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20. člen je bil po mnenju pritožnika kršen, ker so v članku razkriti številni osebni podatki kršitelja, hkrati pa tudi otroka. \n",
      "V prispevku je objavljeno, kje je družina živela, polno ime in priimek očeta, njegove poklicne dejavnosti ter govorice o tem, da je bil v preteklosti že obsojen za kazniva \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the path to the directory containing the files you want to concatenate\n",
    "directory_path = 'cckresV1_0-text'\n",
    "\n",
    "# Initialize an empty string to hold the concatenated content and number of documents\n",
    "concatenated_content = ''\n",
    "doc_num = 1000\n",
    "\n",
    "# Loop through each file in the specified directory\n",
    "for idx, filename in enumerate(os.listdir(directory_path)):\n",
    "    if idx == doc_num:\n",
    "        break\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    \n",
    "    # Check if it is a file (and not a directory/subdirectory)\n",
    "    if os.path.isfile(file_path):\n",
    "        # Open the file for reading\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # Read the file's content and concatenate it\n",
    "            concatenated_content += file.read() + '\\n'  # Adding a newline for separation between files\n",
    "\n",
    "# Optional: Print or save the concatenated content\n",
    "print(concatenated_content[:300])\n",
    "\n",
    "# Optional: To save the concatenated content to a new file:\n",
    "output_file_path = f'concatenated_output_̣{doc_num}.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(concatenated_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8634cec-54a1-42f2-8afd-09dffb3f26ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_content = nltk.word_tokenize(concatenated_content, language='slovene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7c17219-e7be-4d9d-b509-d7eba5031582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/azagar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure you have the 'punkt' tokenizer models downloaded\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc7341d4-db22-4624-9866-e6783f02fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_finder = BigramCollocationFinder.from_words(tokenized_content)\n",
    "\n",
    "# Optionally, you can apply frequency filters\n",
    "bigram_finder.apply_freq_filter(10)\n",
    "\n",
    "bigram_scores = bigram_finder.score_ngrams(BigramAssocMeasures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7995d8b4-991f-432e-bdcf-7a148a1aa2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Bigram Collocations:\n",
      "(('PASTOR', 'MANDERS'), 16.946640486865206)\n",
      "(('TROSNEGA', 'PRAHU'), 16.946640486865206)\n",
      "(('BARVA', 'TROSNEGA'), 16.82110960478135)\n",
      "(('RAZŠIRJENOST', 'Razširjena'), 16.705632387361412)\n",
      "(('Naško', 'Križnar'), 16.406072105502506)\n",
      "(('Ustavna', 'pritožba'), 16.318609264252167)\n",
      "(('ogljikovih', 'hidratov'), 16.318609264252167)\n",
      "(('srednjem', 'veku'), 16.318609264252167)\n",
      "(('Ulica', 'Ristori'), 16.258173410390192)\n",
      "(('Ruske', 'federacije'), 16.236147104060194)\n",
      "(('Zdenka', 'Čebašek'), 16.236147104060194)\n",
      "(('MALO', 'BERILO'), 16.158144592058925)\n",
      "(('vrednostnih', 'papirjev'), 16.120669886640258)\n",
      "(('Black/Process', 'Black'), 16.084144010615145)\n",
      "(('lokalno', 'samoupravo'), 16.08414401061514)\n",
      "(('Evropsko', 'unijo'), 16.013754682723746)\n",
      "(('naribanega', 'parmezana'), 15.917136492555125)\n",
      "(('Black', 'plate'), 15.89149893267275)\n",
      "(('Toneta', 'Čufarja'), 15.762215915727783)\n",
      "(('VELIKOST', 'Klobuk'), 15.762215915727781)\n",
      "(('blagovno', 'znamko'), 15.691826587836383)\n",
      "(('denarno', 'kaznijo'), 15.672717764888679)\n",
      "(('Slovenj', 'Gradec'), 15.602402230252759)\n",
      "(('Urša', 'Peternel'), 15.525653721255178)\n",
      "(('Julijske', 'Alpe'), 15.521207816223987)\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Bigram Collocations:\")\n",
    "for score in bigram_scores[:25]:  # Adjust the slice for the number of collocations you want to see\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db7a2b58-34f4-4b97-9fa1-c90d90adf674",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_finder = TrigramCollocationFinder.from_words(tokenized_content)\n",
    "\n",
    "# Optionally, you can apply frequency filters\n",
    "trigram_finder.apply_freq_filter(10)\n",
    "\n",
    "trigram_scores = trigram_finder.score_ngrams(TrigramAssocMeasures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72ccf8dd-f2e0-4229-8bed-4be0a43a63b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Trigram Collocations:\n",
      "(('BARVA', 'TROSNEGA', 'PRAHU'), 33.76775009164656)\n",
      "(('Black/Process', 'Black', 'plate'), 32.49021611611765)\n",
      "(('Â', 'Â', 'Â'), 30.170256675219033)\n",
      "(('uporabljeni', 'materiali', 'uvrščajo'), 30.07356936397715)\n",
      "(('Ustavna', 'pritožba', 'zoper'), 30.052256027783177)\n",
      "(('novimatajur', '@', 'spin.it'), 29.63846707470159)\n",
      "(('Slovenskem', 'etnografskem', 'muzeju'), 29.52789846117534)\n",
      "(('POGLED', 'OD', 'STRANI'), 29.14745451781312)\n",
      "(('_', '_', '_'), 29.125862555860586)\n",
      "(('mlet', 'črni', 'poper'), 28.897894996230697)\n",
      "(('neprečiščenega', 'olivnega', 'olja'), 28.755863025002952)\n",
      "(('OD', 'STRANI', 'ZGORAJ'), 28.372521073447896)\n",
      "(('sveže', 'mlet', 'črni'), 28.346485055220565)\n",
      "(('izdelka', 'franko', 'tovarna'), 28.18180427040053)\n",
      "(('Državna', 'revizijska', 'komisija'), 27.807518798322256)\n",
      "(('mag.', 'Blaž', 'Kavčič'), 27.494320514450386)\n",
      "(('PRVI', 'IN', 'DRUGI'), 27.24829253392228)\n",
      "(('cene', 'izdelka', 'franko'), 26.799334633578116)\n",
      "(('d.', 'o.', 'o.'), 26.157532371115224)\n",
      "(('vsi', 'uporabljeni', 'materiali'), 25.951703818594662)\n",
      "(('oddaje', 'javnih', 'naročil'), 25.881083559734055)\n",
      "(('Mestne', 'občine', 'Kranj'), 25.562219356044587)\n",
      "(('Uradni', 'list', 'RS'), 25.50753361233572)\n",
      "(('&', '#', '1801810031'), 25.293706197166603)\n",
      "(('oddaje', 'javnega', 'naročila'), 25.261517655218146)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop Trigram Collocations:\")\n",
    "for score in trigram_scores[:25]:  # Adjust the slice for the number of collocations you want to see\n",
    "    print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
